{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# INFO F422 - Statistical Fundation of Machine Learning\n",
    "## Project \"House Prices : Advanced Regression Techniques\"\n",
    "\n",
    "    Erica Berghman\n",
    "    Master 1 - Brussels Engineer School"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction \n",
    "\n",
    "> with dataset description, goals, and an overview of the report structure \n",
    "\n",
    "> Starting from a data set with 81 criteria about houses and their selling price, the goal is to create a model capable of predicting the price of other houses given some of these criterias. A good model description is a model that has been refined multiple types. This report will show the methodology used to construct a model for this particular problem. It is based on the methodology of the Chapter 6 of the syllabus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataSample = 400\n",
    "mean = T          # variable to determine if we use the mean or the median to replace the NA values\n",
    "set.seed(3)\n",
    "\n",
    "source(\"functions/replaceNA.R\")\n",
    "# Hide warnings\n",
    "options(warn=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get a model, the data must be preprocessed. Firstly we read the data given and we take a sample set of 400 houses out of the 1460. There is 81 criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data<-read.csv(\"input/train.csv\")\n",
    "data.sample<-data[sample(nrow(data),dataSample),]\n",
    "#dim(data.sample)\n",
    "#data[1:2,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Categorical criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical (factor) criterias are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "factor_variables<-which(sapply(data.sample[1,],class)==\"factor\")\n",
    "data.sample.nofactor<-data.sample[,-factor_variables]\n",
    "data.sample.factor<-data.sample[,factor_variables]\n",
    "#summary(data.sample.factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of them are then added. (TODO justification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(dummies)\n",
    "variable_to_keep<-c(\"CentralAir\", \"Street\", \"LotShape\")\n",
    "data_factor_onehot <- dummy.data.frame(data.sample.factor[,variable_to_keep], sep=\"_\")\n",
    "data.nofactor.extended<-cbind(data.sample.nofactor,data_factor_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Missing data \n",
    "The missing values (NA) are replaced by an estimator of these values (eg. mean or median)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if (mean) {\n",
    "    data_preprocessed<-data.frame(apply(data.nofactor.extended,2,replace_na_with_mean_value)) \n",
    "} else {\n",
    "    data_preprocessed<-data.frame(apply(data.nofactor.extended,2,replace_na_with_median_value))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature selection \n",
    "> Methodology and main results\n",
    "\n",
    "> The text must contain the list of selected variables and the motivation of their choice. The use of formulas, tables and pseudo-code to describe the feature selection procedure is encouraged. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Redundant and irrelevant features \n",
    "The \"Id\" column which is irrelevant is deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_preprocessed<-data_preprocessed[,setdiff(colnames(data_preprocessed),\"Id\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The criterias that are redundant (linear combination of others criterias and correlation > 0.99) are deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#library(caret)\n",
    "library(ggplot2)\n",
    "library(lattice)\n",
    "\n",
    "#linearCombo.idx <- findLinearCombos(data_preprocessed)$remove\n",
    "#if (!is.null(linearCombo.idx)) data_preprocessed<-data_preprocessed[,-linearCombo.idx]\n",
    "\n",
    "correlation.matrix <- cor(data_preprocessed)\n",
    "correlation.matrix[upper.tri(correlation.matrix)] <- 0\n",
    "diag(correlation.matrix) <- 0\n",
    "data.uncorrelated <- data_preprocessed[,!apply(correlation.matrix,2,function(x) any(abs(x) > 0.99))]\n",
    "dim(data.uncorrelated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input and output vectors are created and scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X <- data.uncorrelated[,setdiff(colnames(data.uncorrelated),\"SalePrice\")]\n",
    "Y <- data.uncorrelated[,\"SalePrice\"]\n",
    "X <- data.frame(X)\n",
    "#Y <- data.frame(Y)\n",
    "X.scale <- data.frame(scale(X))\n",
    "Y.scale <- scale(Y)\n",
    "\n",
    "N<-nrow(X)    #Number of examples\n",
    "n<-ncol(X)    #Number of input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dim(X)\n",
    "X.tr.mean <- colMeans(X)\n",
    "X.tr.std <- apply(X,2,sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Filter methods\n",
    "\n",
    "   It create a subset of features, removing from the whole features set the ones less likely to determine the variable (SalePrice). It is robust to overfitting and effective in computational time. However it might select redundant variables as the interraction between the variables is not taken in consideration.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source(\"functions/filtre.R\")\n",
    "features.filtre <- filtre(X.scale,Y.scale)  # return the idx of the more correlated features where #feature = argmin(CV error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.filtre = X.scale[,features.filtre]\n",
    "dim(X.filtre)\n",
    "X.filtre[1:2,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source(\"functions/mRMR.R\")\n",
    "features.mrmr <- mrmr(X.scale, Y.scale)    # return the idx of the more correlated features where #feature = argmin(CV error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.mrmr <- X.scale[,features.mrmr]\n",
    "dim(X.mrmr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source(\"functions/pca.R\")\n",
    "X.pca <- pca(X.scale, Y.scale)   # return X_pca with nb of columns = argmin(CV error)\n",
    "dim(X.pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Wrapper method\n",
    "\n",
    "Its a cyclic method where a subset of variable is created and evaluated by the Learning Algorithm, modifying the chosen subset. This is done until the best subset is generated.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source(\"functions/wrapper.R\")\n",
    "features.wrapper <- wrapper(X.scale, Y.scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.wrapper <- X[,features.wrapper]\n",
    "dim(X.wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Hybrid method\n",
    "The filter method is used to select a first \"big\" set of features, that is then refined by the wrapper method. This gives us the possibility use advantages of both method to get a good subset in a relatively correct computational time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features.wrapper.pca <- wrapper(X.pca, Y.scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model selection  \n",
    "> Methodology and main results\n",
    "\n",
    "> For the learning method, the only packages that may be used are those seen during the exercise classes : stats, nnet, tree, lazy, and e1071, for linear models, neural networks, decision trees, nearest neighbours and SVM, respectively.\n",
    "\n",
    "> The accuracy of the regression models during the selection process should be assessed by using the root mean squared error between the logarithm of the predicted value and the logarithm of the observed sale price.\n",
    "\n",
    "> The text must mention the different (and at least three) models which have been taken into consideration and the procedure used for model assessment and selection. The use of formulas, tables and pseudo-code to describe the feature selection procedure is encouraged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DS<-cbind(X.scale,SalePrice=Y.scale)\n",
    "model.linear <- lm(SalePrice~.,DS) ### IMDB score given all the other ones (~.) over the dataset DS\n",
    "\n",
    "Y.hat <- predict(model.linear, X.scale)\n",
    "\n",
    "empirical_error<-mean((Y.hat-Y.scale)^2) ### MSE for prediction of that model.\n",
    "print(paste(\"Empirical error=\",round(empirical_error,digits=4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "size.CV<-floor(N/10)\n",
    "\n",
    "CV.err<-numeric(10)\n",
    "\n",
    "for (i in 1:10) {\n",
    "    # 1/10 for testing\n",
    "     i.ts<-(((i-1)*size.CV+1):(i*size.CV))  ### i.ts = indices of the test set for the i-th fold\n",
    "     X.ts<-X[i.ts,]  \n",
    "     Y.ts<-Y[i.ts]    \n",
    "    \n",
    "    #9/10 for training\n",
    "     i.tr<-setdiff(1:N,i.ts)                ###i.tr = indices of the training set for the i-th fold\n",
    "     X.tr<-X[i.tr,]\n",
    "     Y.tr<-Y[i.tr]         \n",
    "     \n",
    "    #scaling\n",
    "     X.tr.mean <- colMeans(X.tr)\n",
    "     X.tr.sd <- apply(X.tr,2,sd)\n",
    "    Y.tr.mean <- mean(Y.tr)\n",
    "    Y.tr.sd <- sd(Y.tr)\n",
    "\n",
    "    Y.tr <- cbind(Y.tr - Y.tr.mean)/Y.tr.sd \n",
    "    X.tr <- t(apply(sweep(X.tr,2,X.tr.mean,\"-\"), 1, function(x) x/X.tr.sd))\n",
    "    \n",
    "    #scaling the testing test by the same scaling as of the training set\n",
    "    X.ts <- t(apply(sweep(X.ts,2,X.tr.mean,\"-\"), 1, function(x) x/X.tr.sd))\n",
    "    Y.ts <- cbind(Y.ts - Y.tr.mean)/Y.tr.sd \n",
    "    \n",
    "    X.ts <-data.frame(X.ts)\n",
    "    X.tr <-data.frame(X.tr)\n",
    "        \n",
    "\n",
    "    DS<-cbind(X.tr,SalePrice=Y.tr)\n",
    "    \n",
    "    model.linear<- lm(SalePrice~.,DS)      # create model with the training set\n",
    "        \n",
    "    Y.hat.ts<- predict(model.linear,X.ts)  # predict value for the test set\n",
    "        \n",
    "    CV.err[i]<-mean((Y.hat.ts-Y.ts)^2)  # MSE for test set\n",
    "}\n",
    "    \n",
    "print(paste(\"CV error=\",round(mean(CV.err),digits=4), \" ; std dev=\",round(sd(CV.err),digits=4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DS<-cbind(X.scale,SalePrice=Y.scale)\n",
    "model.tree <- rpart(SalePrice~.,DS)\n",
    "        \n",
    "Y.hat <- predict(model.tree,X.scale)\n",
    "        \n",
    "empirical_error<-mean((Y.hat-Y.scale)^2) \n",
    "print(paste(\"Empirical error=\",round(empirical_error,digits=4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(rpart.plot)\n",
    "prp(model.tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Decision tree gives a good overview on what criteria are important. It wears its name well as it help you decide the value of the price given a set of caracteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "size.CV<-floor(N/10)\n",
    "\n",
    "CV.err<-numeric(10)\n",
    "\n",
    "for (i in 1:10) {\n",
    "    # 1/10 for testing\n",
    "     i.ts<-(((i-1)*size.CV+1):(i*size.CV))  ### i.ts = indices of the test set for the i-th fold\n",
    "     X.ts<-X[i.ts,]  \n",
    "     Y.ts<-Y[i.ts]    \n",
    "    \n",
    "    #9/10 for training\n",
    "     i.tr<-setdiff(1:N,i.ts)                ###i.tr = indices of the training set for the i-th fold\n",
    "     X.tr<-X[i.tr,]\n",
    "     Y.tr<-Y[i.tr]         \n",
    "     \n",
    "    #scaling\n",
    "     X.tr.mean <- colMeans(X.tr)\n",
    "     X.tr.sd <- apply(X.tr,2,sd)\n",
    "    Y.tr.mean <- mean(Y.tr)\n",
    "    Y.tr.sd <- sd(Y.tr)\n",
    "\n",
    "    Y.tr <- cbind(Y.tr - Y.tr.mean)/Y.tr.sd \n",
    "    X.tr <- t(apply(sweep(X.tr,2,X.tr.mean,\"-\"), 1, function(x) x/X.tr.sd))\n",
    "    \n",
    "    #scaling the testing test by the same scaling as of the training set\n",
    "    X.ts <- t(apply(sweep(X.ts,2,X.tr.mean,\"-\"), 1, function(x) x/X.tr.sd))\n",
    "    Y.ts <- cbind(Y.ts - Y.tr.mean)/Y.tr.sd \n",
    "    \n",
    "    X.ts <-data.frame(X.ts)\n",
    "    X.tr <-data.frame(X.tr)                       \n",
    "     \n",
    "     DS<-cbind(X.tr,SalePrice=Y.tr)\n",
    "    \n",
    "     model<- rpart(SalePrice~.,DS)\n",
    "        \n",
    "     Y.hat.ts<- predict(model.tree,X.ts)\n",
    "        \n",
    "     CV.err[i]<-mean((Y.hat.ts-Y.ts)^2)\n",
    "    }\n",
    "    \n",
    "\n",
    "print(paste(\"CV error =\",round(mean(CV.err),digits=4), \" std dev =\",round(sd(CV.err),digits=4)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?nnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(nnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DS<-cbind(X.scale,SalePrice=Y.scale)\n",
    "for (i in 5:22){\n",
    "    #model.neural[i] <- nnet(SalePrice~.,DS, size = i, linout=T, maxit = 1000)\n",
    "    Y.hat <- predict(nnet(SalePrice~.,DS, size = i, linout=T, maxit = 1000),X.scale)\n",
    "\n",
    "    empirical_error[i] <- mean((Y.hat-Y.scale)^2) \n",
    "    print(paste(\"Number of neurones: \", i, \"Empirical error = \",round(empirical_error[i],digits=4)))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter \"size\"\n",
    "An analysis of the number of neurones is done. The maximum of neurones used is 20 as more return an error (too many weights).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#juste un save pour les tests\n",
    "empirical_error <- c(0.0143671130358607, 0.015019445363705, 0.00821834907673351, 0.0054200745645435, 0.00142228029226354, 5.25161597874794e-05, 1.52899010549121e-06, 2.47317346679477e-07, 2.37597648006074e-07, 2.47841099771862e-07, 2.26350153783801e-07, 2.30675129791945e-07, 2.44205643742954e-07, 2.4713486169669e-07, 2.40667851762e-07, 2.31459671090645e-07, 2.39175596505192e-07, 2.24902711484879e-07) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(c(5:22),empirical_error, xlab = \"Number of neurones\", ylab = \"Empirical error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first view it seems like a size > 10 gives the best result. As the computational time incrases with the number of neurones, its important to find the smallest size that gives good result. We will now asses the 10-fold cross validation for the 10 models (size 10 to 20) to determine if they are good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot(c(12:22),empirical_error[8:18], xlab = \"Number of neurones\", ylab = \"Empirical error\")  # zoom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean.size<-numeric(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "size.CV<-floor(N/10)\n",
    "\n",
    "CV.err<-numeric(10)\n",
    "\n",
    "for (j in 10:20){\n",
    "    \n",
    "    for (i in 1:10) {\n",
    "        # 1/10 for testing\n",
    "         i.ts<-(((i-1)*size.CV+1):(i*size.CV))  ### i.ts = indices of the test set for the i-th fold\n",
    "         X.ts<-X[i.ts,]  \n",
    "         Y.ts<-Y[i.ts]    \n",
    "\n",
    "        #9/10 for training\n",
    "         i.tr<-setdiff(1:N,i.ts)                ###i.tr = indices of the training set for the i-th fold\n",
    "         X.tr<-X[i.tr,]\n",
    "         Y.tr<-Y[i.tr]         \n",
    "\n",
    "        #scaling\n",
    "         X.tr.mean <- colMeans(X.tr)\n",
    "         X.tr.sd <- apply(X.tr,2,sd)\n",
    "        Y.tr.mean <- mean(Y.tr)\n",
    "        Y.tr.sd <- sd(Y.tr)\n",
    "\n",
    "        Y.tr <- cbind(Y.tr - Y.tr.mean)/Y.tr.sd \n",
    "        X.tr <- t(apply(sweep(X.tr,2,X.tr.mean,\"-\"), 1, function(x) x/X.tr.sd))\n",
    "\n",
    "        #scaling the testing test by the same scaling as of the training set\n",
    "        X.ts <- t(apply(sweep(X.ts,2,X.tr.mean,\"-\"), 1, function(x) x/X.tr.sd))\n",
    "        Y.ts <- cbind(Y.ts - Y.tr.mean)/Y.tr.sd \n",
    "\n",
    "        X.ts <-data.frame(X.ts)\n",
    "        X.tr <-data.frame(X.tr)                       \n",
    "\n",
    "         DS <- cbind(X.tr,SalePrice=Y.tr)\n",
    "        \n",
    "         model.neural <- nnet(SalePrice~.,DS, size = j, linout=T, maxit = 1000)\n",
    "\n",
    "         Y.hat.ts<- predict(model.neural,X.ts)\n",
    "\n",
    "         CV.err[i]<-mean((Y.hat.ts-Y.ts)^2)\n",
    "        }\n",
    "    mean.size[j] <- round( mean(CV.err),digits=4 )\n",
    "    print(paste(\"Number of neurones: \", j, \" CV error = \",mean.size[j-9], \" std dev = \",round(sd(CV.err),digits=4)))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(c(10:20), mean.size(10:20), xlabel = \"Number of neurones\" , ylabel= \"Cross validation error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the CV error is far from the ~0 we had before. This is because the model was overfitting the set. Let's see what kind of results we get for the CV with a smaller network size (<9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "size.CV<-floor(N/10)\n",
    "\n",
    "for (j in 1:9){\n",
    "    \n",
    "    for (i in 1:10) {\n",
    "        # 1/10 for testing\n",
    "         i.ts<-(((i-1)*size.CV+1):(i*size.CV))  ### i.ts = indices of the test set for the i-th fold\n",
    "         X.ts<-X[i.ts,]  \n",
    "         Y.ts<-Y[i.ts]    \n",
    "\n",
    "        #9/10 for training\n",
    "         i.tr<-setdiff(1:N,i.ts)                ###i.tr = indices of the training set for the i-th fold\n",
    "         X.tr<-X[i.tr,]\n",
    "         Y.tr<-Y[i.tr]         \n",
    "\n",
    "        #scaling\n",
    "         X.tr.mean <- colMeans(X.tr)\n",
    "         X.tr.sd <- apply(X.tr,2,sd)\n",
    "        Y.tr.mean <- mean(Y.tr)\n",
    "        Y.tr.sd <- sd(Y.tr)\n",
    "\n",
    "        Y.tr <- cbind(Y.tr - Y.tr.mean)/Y.tr.sd \n",
    "        X.tr <- t(apply(sweep(X.tr,2,X.tr.mean,\"-\"), 1, function(x) x/X.tr.sd))\n",
    "\n",
    "        #scaling the testing test by the same scaling as of the training set\n",
    "        X.ts <- t(apply(sweep(X.ts,2,X.tr.mean,\"-\"), 1, function(x) x/X.tr.sd))\n",
    "        Y.ts <- cbind(Y.ts - Y.tr.mean)/Y.tr.sd \n",
    "\n",
    "        X.ts <-data.frame(X.ts)\n",
    "        X.tr <-data.frame(X.tr)                       \n",
    "\n",
    "         DS <- cbind(X.tr,SalePrice=Y.tr)\n",
    "        \n",
    "         model.neural <- nnet(SalePrice~.,DS, size = j, linout=T, maxit = 1000)\n",
    "\n",
    "         Y.hat.ts<- predict(model.neural,X.ts)\n",
    "\n",
    "         CV.err[i]<-mean((Y.hat.ts-Y.ts)^2)\n",
    "        }\n",
    "    mean.size[j] <- round( mean(CV.err),digits=4 )\n",
    "    print(paste(\"Number of neurones: \", j, \" CV error = \",mean.size[j], \" std dev = \",round(sd(CV.err),digits=4)))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(c(3:9), mean.size[1:9], xlabel = \"Number of neurones\" , ylabel= \"Cross validation error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean.size[1:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble techniques : Combination of models strategy\n",
    "> Methodology and main results\n",
    "\n",
    "> The text should mention the different models taken into consideration as well as the techniques used for the combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size.CV<-floor(N/10)\n",
    "R<-20   # R models\n",
    "\n",
    "CV.err<-numeric(10)\n",
    "\n",
    "for (i in 1:10) {\n",
    "     i.ts<-(((i-1)*size.CV+1):(i*size.CV))  \n",
    "     X.ts<-X[i.ts,]  \n",
    "     Y.ts<-Y[i.ts]  \n",
    "     \n",
    "     \n",
    "     i.tr<-setdiff(1:N,i.ts)                \n",
    "    \n",
    "     Y.hat.ts.R<-matrix(0,nrow=nrow(X.ts),ncol=R)\n",
    "    \n",
    "     for (r in 1:R) {\n",
    "         i.tr.resample<-sample(i.tr,rep=T)  #rep = replace\n",
    "         X.tr<-X[i.tr.resample,]\n",
    "         Y.tr<-Y[i.tr.resample]                          \n",
    "     \n",
    "         DS<-cbind(X.tr,SalePrice=Y.tr)\n",
    "    \n",
    "         model<- lm(SalePrice~.,DS)\n",
    "        \n",
    "         Y.hat.ts.R[,r]<- predict(model,X.ts)\n",
    "     \n",
    "     }\n",
    "    \n",
    "     Y.hat.ts<-apply(Y.hat.ts.R,1,mean) \n",
    "     CV.err[i]<-mean((Y.hat.ts-Y.ts)^2)\n",
    "     }\n",
    "\n",
    "print(paste(\"CV error=\",round(mean(CV.err),digits=4), \" ; std dev=\",round(sd(CV.err),digits=4)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion and conclusion: \n",
    "> Summary of your work, and discussion of what worked well, not well, why, what insights you got from the analyses you made. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R 3.3",
   "language": "R",
   "name": "ir33"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
