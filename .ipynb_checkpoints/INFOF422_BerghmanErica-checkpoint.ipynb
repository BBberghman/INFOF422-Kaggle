{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# INFO F422 - Statistical Fundation of Machine Learning\n",
    "## Project \"House Prices : Advanced Regression Techniques\"\n",
    "\n",
    "    Erica Berghman\n",
    "    Master 1 - Brussels Engineer School"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction \n",
    "\n",
    "> with dataset description, goals, and an overview of the report structure \n",
    "\n",
    "> Starting from a data set with 81 criteria about houses and their selling price, the goal is to create a model capable of predicting the price of other houses given some of these criterias. A good model description is a model that has been refined multiple types. This report will show the methodology used to construct a model for this particular problem. It is based on the methodology of the Chapter 6 of the syllabus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataSample = 400\n",
    "mean = T          # variable to determine if we use the mean or the median to replace the NA values\n",
    "set.seed(1)\n",
    "\n",
    "source(\"functions/replaceNA.R\")\n",
    "# Hide warnings\n",
    "options(warn=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get a model, the data must be preprocessed. Firstly we read the data given and we take a sample set of 400 houses out of the 1460. There is 81 criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data<-read.csv(\"input/train.csv\")\n",
    "data.sample<-data[sample(nrow(data),dataSample),]\n",
    "#dim(data.sample)\n",
    "#data[1:2,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Categorical criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical (factor) criterias are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "factor_variables<-which(sapply(data.sample[1,],class)==\"factor\")\n",
    "data.sample.nofactor<-data.sample[,-factor_variables]\n",
    "data.sample.factor<-data.sample[,factor_variables]\n",
    "#summary(data.sample.factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of them are then added. (TODO justification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dummies-1.5.6 provided by Decision Patterns\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(dummies)\n",
    "variable_to_keep<-c(\"CentralAir\", \"Street\", \"LotShape\")\n",
    "data_factor_onehot <- dummy.data.frame(data.sample.factor[,variable_to_keep], sep=\"_\")\n",
    "data.nofactor.extended<-cbind(data.sample.nofactor,data_factor_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Missing data \n",
    "The missing values (NA) are replaced by an estimator of these values (eg. mean or median)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if (mean) {\n",
    "    data_preprocessed<-data.frame(apply(data.nofactor.extended,2,replace_na_with_mean_value)) \n",
    "} else {\n",
    "    data_preprocessed<-data.frame(apply(data.nofactor.extended,2,replace_na_with_median_value))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature selection \n",
    "> Methodology and main results\n",
    "\n",
    "> The text must contain the list of selected variables and the motivation of their choice. The use of formulas, tables and pseudo-code to describe the feature selection procedure is encouraged. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Redundant and irrelevant features \n",
    "The \"Id\" column which is irrelevant is deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_preprocessed<-data_preprocessed[,setdiff(colnames(data_preprocessed),\"Id\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The criterias that are redundant (linear combination of others criterias and correlation > 0.99) are deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>400</li>\n",
       "\t<li>43</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 400\n",
       "\\item 43\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 400\n",
       "2. 43\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 400  43"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#library(caret)\n",
    "library(ggplot2)\n",
    "library(lattice)\n",
    "\n",
    "#linearCombo.idx <- findLinearCombos(data_preprocessed)$remove\n",
    "#if (!is.null(linearCombo.idx)) data_preprocessed<-data_preprocessed[,-linearCombo.idx]\n",
    "\n",
    "correlation.matrix <- cor(data_preprocessed)\n",
    "correlation.matrix[upper.tri(correlation.matrix)] <- 0\n",
    "diag(correlation.matrix) <- 0\n",
    "data.uncorrelated <- data_preprocessed[,!apply(correlation.matrix,2,function(x) any(abs(x) > 0.99))]\n",
    "dim(data.uncorrelated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input and output vectors are created and scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X <- data.uncorrelated[,setdiff(colnames(data.uncorrelated),\"SalePrice\")]\n",
    "Y <- data.uncorrelated[,\"SalePrice\"]\n",
    "X <- data.frame(X)\n",
    "#Y <- data.frame(Y)\n",
    "X.scale <- data.frame(scale(X))\n",
    "Y.scale <- scale(Y)\n",
    "\n",
    "N<-nrow(X)    #Number of examples\n",
    "n<-ncol(X)    #Number of input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>400</li>\n",
       "\t<li>42</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 400\n",
       "\\item 42\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 400\n",
       "2. 42\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 400  42"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim(X)\n",
    "X.tr.mean <- colMeans(X)\n",
    "X.tr.std <- apply(X,2,sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Filter methods\n",
    "\n",
    "   It create a subset of features, removing from the whole features set the ones less likely to determine the variable (SalePrice). It is robust to overfitting and effective in computational time. However it might select redundant variables as the interraction between the variables is not taken in consideration.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source(\"functions/filtre.R\")\n",
    "features.filtre <- filtre(X.scale,Y.scale)  # return the idx of the more correlated features where #feature = argmin(CV error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>400</li>\n",
       "\t<li>35</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 400\n",
       "\\item 35\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 400\n",
       "2. 35\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 400  35"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>OverallQual</th><th scope=col>GrLivArea</th><th scope=col>TotalBsmtSF</th><th scope=col>X1stFlrSF</th><th scope=col>GarageCars</th><th scope=col>GarageArea</th><th scope=col>FullBath</th><th scope=col>YearRemodAdd</th><th scope=col>TotRmsAbvGrd</th><th scope=col>YearBuilt</th><th scope=col>...</th><th scope=col>BsmtUnfSF</th><th scope=col>MSSubClass</th><th scope=col>BedroomAbvGr</th><th scope=col>KitchenAbvGr</th><th scope=col>EnclosedPorch</th><th scope=col>ScreenPorch</th><th scope=col>X3SsnPorch</th><th scope=col>MiscVal</th><th scope=col>OverallCond</th><th scope=col>LotShape_IR2</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>388</th><td>-0.09423743</td><td>-0.7470902 </td><td>-0.06201711</td><td>-0.108574  </td><td>-0.9763774 </td><td>-0.5683105 </td><td>-1.0707010 </td><td>-0.4480077 </td><td>-0.3212315 </td><td>0.1227521  </td><td>...        </td><td>-0.3683924 </td><td> 0.6140872 </td><td>0.1035     </td><td>-0.2291288 </td><td>-0.3559642 </td><td>-0.2798042 </td><td>-0.117066  </td><td>-0.1506729 </td><td> 0.3556585 </td><td>-0.1599279 </td></tr>\n",
       "\t<tr><th scope=row>543</th><td> 0.59112569</td><td> 0.2952237 </td><td> 1.38021372</td><td> 1.346596  </td><td> 0.2753885 </td><td> 0.4394986 </td><td> 0.7595571 </td><td> 0.6441150 </td><td> 0.2906380 </td><td>0.8516444  </td><td>...        </td><td>-0.4352562 </td><td>-0.8436866 </td><td>0.1035     </td><td>-0.2291288 </td><td>-0.3559642 </td><td>-0.2798042 </td><td>-0.117066  </td><td>-0.1506729 </td><td>-0.4811850 </td><td>-0.1599279 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllllllllllllllllllllllllllllll}\n",
       "  & OverallQual & GrLivArea & TotalBsmtSF & X1stFlrSF & GarageCars & GarageArea & FullBath & YearRemodAdd & TotRmsAbvGrd & YearBuilt & ... & BsmtUnfSF & MSSubClass & BedroomAbvGr & KitchenAbvGr & EnclosedPorch & ScreenPorch & X3SsnPorch & MiscVal & OverallCond & LotShape\\_IR2\\\\\n",
       "\\hline\n",
       "\t388 & -0.09423743 & -0.7470902  & -0.06201711 & -0.108574   & -0.9763774  & -0.5683105  & -1.0707010  & -0.4480077  & -0.3212315  & 0.1227521   & ...         & -0.3683924  &  0.6140872  & 0.1035      & -0.2291288  & -0.3559642  & -0.2798042  & -0.117066   & -0.1506729  &  0.3556585  & -0.1599279 \\\\\n",
       "\t543 &  0.59112569 &  0.2952237  &  1.38021372 &  1.346596   &  0.2753885  &  0.4394986  &  0.7595571  &  0.6441150  &  0.2906380  & 0.8516444   & ...         & -0.4352562  & -0.8436866  & 0.1035      & -0.2291288  & -0.3559642  & -0.2798042  & -0.117066   & -0.1506729  & -0.4811850  & -0.1599279 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | OverallQual | GrLivArea | TotalBsmtSF | X1stFlrSF | GarageCars | GarageArea | FullBath | YearRemodAdd | TotRmsAbvGrd | YearBuilt | ... | BsmtUnfSF | MSSubClass | BedroomAbvGr | KitchenAbvGr | EnclosedPorch | ScreenPorch | X3SsnPorch | MiscVal | OverallCond | LotShape_IR2 | \n",
       "|---|---|\n",
       "| 388 | -0.09423743 | -0.7470902  | -0.06201711 | -0.108574   | -0.9763774  | -0.5683105  | -1.0707010  | -0.4480077  | -0.3212315  | 0.1227521   | ...         | -0.3683924  |  0.6140872  | 0.1035      | -0.2291288  | -0.3559642  | -0.2798042  | -0.117066   | -0.1506729  |  0.3556585  | -0.1599279  | \n",
       "| 543 |  0.59112569 |  0.2952237  |  1.38021372 |  1.346596   |  0.2753885  |  0.4394986  |  0.7595571  |  0.6441150  |  0.2906380  | 0.8516444   | ...         | -0.4352562  | -0.8436866  | 0.1035      | -0.2291288  | -0.3559642  | -0.2798042  | -0.117066   | -0.1506729  | -0.4811850  | -0.1599279  | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "    OverallQual GrLivArea  TotalBsmtSF X1stFlrSF GarageCars GarageArea\n",
       "388 -0.09423743 -0.7470902 -0.06201711 -0.108574 -0.9763774 -0.5683105\n",
       "543  0.59112569  0.2952237  1.38021372  1.346596  0.2753885  0.4394986\n",
       "    FullBath   YearRemodAdd TotRmsAbvGrd YearBuilt ... BsmtUnfSF  MSSubClass\n",
       "388 -1.0707010 -0.4480077   -0.3212315   0.1227521 ... -0.3683924  0.6140872\n",
       "543  0.7595571  0.6441150    0.2906380   0.8516444 ... -0.4352562 -0.8436866\n",
       "    BedroomAbvGr KitchenAbvGr EnclosedPorch ScreenPorch X3SsnPorch MiscVal   \n",
       "388 0.1035       -0.2291288   -0.3559642    -0.2798042  -0.117066  -0.1506729\n",
       "543 0.1035       -0.2291288   -0.3559642    -0.2798042  -0.117066  -0.1506729\n",
       "    OverallCond LotShape_IR2\n",
       "388  0.3556585  -0.1599279  \n",
       "543 -0.4811850  -0.1599279  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X.filtre = X.scale[,features.filtre]\n",
    "dim(X.filtre)\n",
    "X.filtre[1:2,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source(\"functions/mRMR.R\")\n",
    "features.mrmr <- mrmr(X.scale, Y.scale)    # return the idx of the more correlated features where #feature = argmin(CV error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>400</li>\n",
       "\t<li>40</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 400\n",
       "\\item 40\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 400\n",
       "2. 40\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 400  40"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X.mrmr <- X.scale[,features.mrmr]\n",
    "dim(X.mrmr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>400</li>\n",
       "\t<li>40</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 400\n",
       "\\item 40\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 400\n",
       "2. 40\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 400  40"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "source(\"functions/pca.R\")\n",
    "X.pca <- pca(X.scale, Y.scale)   # return X_pca with nb of columns = argmin(CV error)\n",
    "dim(X.pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Wrapper method\n",
    "\n",
    "Its a cyclic method where a subset of variable is created and evaluated by the Learning Algorithm, modifying the chosen subset. This is done until the best subset is generated.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in source(\"functions/wrapper.R\"): functions/wrapper.R:45:5: unexpected symbol\n44:     nb_features.best <- which.min(round(CV.err.mean[selected_current],digits=4)\n45:     print\n        ^\n",
     "output_type": "error",
     "traceback": [
      "Error in source(\"functions/wrapper.R\"): functions/wrapper.R:45:5: unexpected symbol\n44:     nb_features.best <- which.min(round(CV.err.mean[selected_current],digits=4)\n45:     print\n        ^\nTraceback:\n",
      "1. source(\"functions/wrapper.R\")"
     ]
    }
   ],
   "source": [
    "source(\"functions/wrapper.R\")\n",
    "features.wrapper <- wrapper(X.scale, Y.scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.wrapper <- X[,features.wrapper]\n",
    "dim(X.wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Hybrid method\n",
    "The filter method is used to select a first \"big\" set of features, that is then refined by the wrapper method. This gives us the possibility use advantages of both method to get a good subset in a relatively correct computational time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features.wrapper.pca <- wrapper(X.pca, Y.scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model selection  \n",
    "> Methodology and main results\n",
    "\n",
    "> For the learning method, the only packages that may be used are those seen during the exercise classes : stats, nnet, tree, lazy, and e1071, for linear models, neural networks, decision trees, nearest neighbours and SVM, respectively.\n",
    "\n",
    "> The accuracy of the regression models during the selection process should be assessed by using the root mean squared error between the logarithm of the predicted value and the logarithm of the observed sale price.\n",
    "\n",
    "> The text must mention the different (and at least three) models which have been taken into consideration and the procedure used for model assessment and selection. The use of formulas, tables and pseudo-code to describe the feature selection procedure is encouraged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Empirical error= 0.0916\"\n"
     ]
    }
   ],
   "source": [
    "DS<-cbind(X.scale,SalePrice=Y.scale)\n",
    "model.linear <- lm(SalePrice~.,DS) ### IMDB score given all the other ones (~.) over the dataset DS\n",
    "\n",
    "Y.hat <- predict(model.linear, X.scale)\n",
    "\n",
    "empirical_error<-mean((Y.hat-Y.scale)^2) ### MSE for prediction of that model.\n",
    "print(paste(\"Empirical error=\",round(empirical_error,digits=4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"CV error= 0.1744  ; std dev= 0.1945\"\n"
     ]
    }
   ],
   "source": [
    "size.CV<-floor(N/10)\n",
    "\n",
    "CV.err<-numeric(10)\n",
    "\n",
    "for (i in 1:10) {\n",
    "    # 1/10 for testing\n",
    "     i.ts<-(((i-1)*size.CV+1):(i*size.CV))  ### i.ts = indices of the test set for the i-th fold\n",
    "     X.ts<-X[i.ts,]  \n",
    "     Y.ts<-Y[i.ts]    \n",
    "    \n",
    "    #9/10 for training\n",
    "     i.tr<-setdiff(1:N,i.ts)                ###i.tr = indices of the training set for the i-th fold\n",
    "     X.tr<-X[i.tr,]\n",
    "     Y.tr<-Y[i.tr]         \n",
    "     \n",
    "    #scaling\n",
    "     X.tr.mean <- colMeans(X.tr)\n",
    "     X.tr.sd <- apply(X.tr,2,sd)\n",
    "    Y.tr.mean <- mean(Y.tr)\n",
    "    Y.tr.sd <- sd(Y.tr)\n",
    "\n",
    "    Y.tr <- cbind(Y.tr - Y.tr.mean)/Y.tr.sd \n",
    "    X.tr <- t(apply(sweep(X.tr,2,X.tr.mean,\"-\"), 1, function(x) x/X.tr.sd))\n",
    "    \n",
    "    #scaling the testing test by the same scaling as of the training set\n",
    "    X.ts <- t(apply(sweep(X.ts,2,X.tr.mean,\"-\"), 1, function(x) x/X.tr.sd))\n",
    "    Y.ts <- cbind(Y.ts - Y.tr.mean)/Y.tr.sd \n",
    "    \n",
    "    X.ts <-data.frame(X.ts)\n",
    "    X.tr <-data.frame(X.tr)\n",
    "        \n",
    "\n",
    "    DS<-cbind(X.tr,SalePrice=Y.tr)\n",
    "    \n",
    "    model.linear<- lm(SalePrice~.,DS)      # create model with the training set\n",
    "        \n",
    "    Y.hat.ts<- predict(model.linear,X.ts)  # predict value for the test set\n",
    "        \n",
    "    CV.err[i]<-mean((Y.hat.ts-Y.ts)^2)  # MSE for test set\n",
    "}\n",
    "    \n",
    "print(paste(\"CV error=\",round(mean(CV.err),digits=4), \" ; std dev=\",round(sd(CV.err),digits=4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Empirical error= 0.1923\"\n"
     ]
    }
   ],
   "source": [
    "DS<-cbind(X.scale,SalePrice=Y.scale)\n",
    "model.tree <- rpart(SalePrice~.,DS)\n",
    "        \n",
    "Y.hat <- predict(model.tree,X.scale)\n",
    "        \n",
    "empirical_error<-mean((Y.hat-Y.scale)^2) \n",
    "print(paste(\"Empirical error=\",round(empirical_error,digits=4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(rpart.plot)\n",
    "prp(model.tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"CV error = 0.2325  std dev = 0.2906\"\n"
     ]
    }
   ],
   "source": [
    "size.CV<-floor(N/10)\n",
    "\n",
    "CV.err<-numeric(10)\n",
    "\n",
    "for (i in 1:10) {\n",
    "    # 1/10 for testing\n",
    "     i.ts<-(((i-1)*size.CV+1):(i*size.CV))  ### i.ts = indices of the test set for the i-th fold\n",
    "     X.ts<-X[i.ts,]  \n",
    "     Y.ts<-Y[i.ts]    \n",
    "    \n",
    "    #9/10 for training\n",
    "     i.tr<-setdiff(1:N,i.ts)                ###i.tr = indices of the training set for the i-th fold\n",
    "     X.tr<-X[i.tr,]\n",
    "     Y.tr<-Y[i.tr]         \n",
    "     \n",
    "    #scaling\n",
    "     X.tr.mean <- colMeans(X.tr)\n",
    "     X.tr.sd <- apply(X.tr,2,sd)\n",
    "    Y.tr.mean <- mean(Y.tr)\n",
    "    Y.tr.sd <- sd(Y.tr)\n",
    "\n",
    "    Y.tr <- cbind(Y.tr - Y.tr.mean)/Y.tr.sd \n",
    "    X.tr <- t(apply(sweep(X.tr,2,X.tr.mean,\"-\"), 1, function(x) x/X.tr.sd))\n",
    "    \n",
    "    #scaling the testing test by the same scaling as of the training set\n",
    "    X.ts <- t(apply(sweep(X.ts,2,X.tr.mean,\"-\"), 1, function(x) x/X.tr.sd))\n",
    "    Y.ts <- cbind(Y.ts - Y.tr.mean)/Y.tr.sd \n",
    "    \n",
    "    X.ts <-data.frame(X.ts)\n",
    "    X.tr <-data.frame(X.tr)                       \n",
    "     \n",
    "     DS<-cbind(X.tr,SalePrice=Y.tr)\n",
    "    \n",
    "     model<- rpart(SalePrice~.,DS)\n",
    "        \n",
    "     Y.hat.ts<- predict(model.tree,X.ts)\n",
    "        \n",
    "     CV.err[i]<-mean((Y.hat.ts-Y.ts)^2)\n",
    "    }\n",
    "    \n",
    "\n",
    "print(paste(\"CV error =\",round(mean(CV.err),digits=4), \" std dev =\",round(sd(CV.err),digits=4)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?nnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(nnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# weights:  221\n",
      "initial  value 482.169307 \n",
      "iter  10 value 83.591382\n",
      "iter  20 value 51.205708\n",
      "iter  30 value 42.132795\n",
      "iter  40 value 34.754309\n",
      "iter  50 value 27.572751\n",
      "iter  60 value 23.183973\n",
      "iter  70 value 17.539671\n",
      "iter  80 value 13.653254\n",
      "iter  90 value 11.221008\n",
      "iter 100 value 9.986719\n",
      "iter 110 value 9.071293\n",
      "iter 120 value 8.476875\n",
      "iter 130 value 8.073844\n",
      "iter 140 value 7.895714\n",
      "iter 150 value 7.734080\n",
      "iter 160 value 7.516339\n",
      "iter 170 value 7.229372\n",
      "iter 180 value 6.998418\n",
      "iter 190 value 6.731336\n",
      "iter 200 value 6.541693\n",
      "iter 210 value 6.253488\n",
      "iter 220 value 6.107651\n",
      "iter 230 value 6.039122\n",
      "iter 240 value 5.995854\n",
      "iter 250 value 5.895951\n",
      "iter 260 value 5.858728\n",
      "iter 270 value 5.828831\n",
      "iter 280 value 5.814106\n",
      "iter 290 value 5.797464\n",
      "iter 300 value 5.790562\n",
      "iter 310 value 5.783974\n",
      "iter 320 value 5.779930\n",
      "iter 330 value 5.778029\n",
      "iter 340 value 5.776181\n",
      "iter 350 value 5.773637\n",
      "iter 360 value 5.771632\n",
      "iter 370 value 5.770405\n",
      "iter 380 value 5.768526\n",
      "iter 390 value 5.766889\n",
      "iter 400 value 5.766267\n",
      "iter 410 value 5.765786\n",
      "iter 420 value 5.765463\n",
      "iter 430 value 5.765015\n",
      "iter 440 value 5.764620\n",
      "iter 450 value 5.764397\n",
      "iter 460 value 5.764375\n",
      "iter 470 value 5.764345\n",
      "iter 480 value 5.764312\n",
      "iter 490 value 5.764284\n",
      "iter 500 value 5.764252\n",
      "iter 510 value 5.764223\n",
      "iter 520 value 5.764192\n",
      "iter 530 value 5.764157\n",
      "iter 540 value 5.764116\n",
      "iter 550 value 5.764022\n",
      "iter 560 value 5.763942\n",
      "iter 570 value 5.763473\n",
      "iter 580 value 5.763093\n",
      "iter 590 value 5.762837\n",
      "iter 600 value 5.762603\n",
      "iter 610 value 5.760820\n",
      "iter 620 value 5.758963\n",
      "iter 630 value 5.758125\n",
      "iter 640 value 5.757631\n",
      "iter 650 value 5.757535\n",
      "iter 660 value 5.757366\n",
      "iter 670 value 5.757244\n",
      "iter 680 value 5.756931\n",
      "iter 690 value 5.756606\n",
      "iter 700 value 5.756418\n",
      "iter 710 value 5.755975\n",
      "iter 720 value 5.755957\n",
      "iter 730 value 5.755902\n",
      "iter 740 value 5.755799\n",
      "iter 750 value 5.755705\n",
      "iter 760 value 5.755654\n",
      "iter 770 value 5.755621\n",
      "iter 780 value 5.755580\n",
      "iter 790 value 5.755535\n",
      "iter 800 value 5.755368\n",
      "iter 810 value 5.754980\n",
      "iter 820 value 5.753896\n",
      "iter 830 value 5.753280\n",
      "iter 840 value 5.752944\n",
      "iter 850 value 5.752651\n",
      "iter 860 value 5.752371\n",
      "iter 870 value 5.751734\n",
      "iter 880 value 5.751577\n",
      "iter 890 value 5.751419\n",
      "iter 900 value 5.751377\n",
      "iter 910 value 5.751337\n",
      "iter 920 value 5.751315\n",
      "iter 930 value 5.751271\n",
      "iter 940 value 5.751208\n",
      "iter 950 value 5.751060\n",
      "iter 960 value 5.750898\n",
      "iter 970 value 5.749939\n",
      "iter 980 value 5.749343\n",
      "iter 990 value 5.748481\n",
      "iter1000 value 5.746845\n",
      "final  value 5.746845 \n",
      "stopped after 1000 iterations\n",
      "[1] \"Number of neurones:  5 Empirical error =  0.0144\"\n",
      "# weights:  265\n",
      "initial  value 1662.200644 \n",
      "iter  10 value 133.345214\n",
      "iter  20 value 69.588125\n",
      "iter  30 value 56.042999\n",
      "iter  40 value 46.982583\n",
      "iter  50 value 39.729988\n",
      "iter  60 value 35.633858\n",
      "iter  70 value 31.973529\n",
      "iter  80 value 28.195745\n",
      "iter  90 value 24.166454\n",
      "iter 100 value 22.160548\n",
      "iter 110 value 20.502928\n",
      "iter 120 value 18.038169\n",
      "iter 130 value 16.411232\n",
      "iter 140 value 14.424100\n",
      "iter 150 value 13.169828\n",
      "iter 160 value 12.384060\n",
      "iter 170 value 11.633043\n",
      "iter 180 value 10.679717\n",
      "iter 190 value 10.214582\n",
      "iter 200 value 9.722144\n",
      "iter 210 value 9.393015\n",
      "iter 220 value 9.121836\n",
      "iter 230 value 8.870295\n",
      "iter 240 value 8.642969\n",
      "iter 250 value 8.351857\n",
      "iter 260 value 8.026752\n",
      "iter 270 value 7.531980\n",
      "iter 280 value 7.274055\n",
      "iter 290 value 7.193423\n",
      "iter 300 value 7.120894\n",
      "iter 310 value 7.048352\n",
      "iter 320 value 6.931438\n",
      "iter 330 value 6.851061\n",
      "iter 340 value 6.782113\n",
      "iter 350 value 6.728061\n",
      "iter 360 value 6.656642\n",
      "iter 370 value 6.569288\n",
      "iter 380 value 6.528188\n",
      "iter 390 value 6.508608\n",
      "iter 400 value 6.489230\n",
      "iter 410 value 6.480436\n",
      "iter 420 value 6.464486\n",
      "iter 430 value 6.446421\n",
      "iter 440 value 6.413848\n",
      "iter 450 value 6.369769\n",
      "iter 460 value 6.339638\n",
      "iter 470 value 6.316651\n",
      "iter 480 value 6.301440\n",
      "iter 490 value 6.293090\n",
      "iter 500 value 6.288942\n",
      "iter 510 value 6.284119\n",
      "iter 520 value 6.278021\n",
      "iter 530 value 6.251268\n",
      "iter 540 value 6.248704\n",
      "iter 550 value 6.247743\n",
      "iter 560 value 6.245827\n",
      "iter 570 value 6.243976\n",
      "iter 580 value 6.242240\n",
      "iter 590 value 6.239788\n",
      "iter 600 value 6.237041\n",
      "iter 610 value 6.233709\n",
      "iter 620 value 6.228770\n",
      "iter 630 value 6.220412\n",
      "iter 640 value 6.214527\n",
      "iter 650 value 6.207487\n",
      "iter 660 value 6.203185\n",
      "iter 670 value 6.199264\n",
      "iter 680 value 6.195234\n",
      "iter 690 value 6.187090\n",
      "iter 700 value 6.183282\n",
      "iter 710 value 6.179015\n",
      "iter 720 value 6.177697\n",
      "iter 730 value 6.177032\n",
      "iter 740 value 6.176492\n",
      "iter 750 value 6.175826\n",
      "iter 760 value 6.174828\n",
      "iter 770 value 6.172328\n",
      "iter 780 value 6.167831\n",
      "iter 790 value 6.155578\n",
      "iter 800 value 6.147771\n",
      "iter 810 value 6.142267\n",
      "iter 820 value 6.139004\n",
      "iter 830 value 6.137324\n",
      "iter 840 value 6.133413\n",
      "iter 850 value 6.123251\n",
      "iter 860 value 6.096544\n",
      "iter 870 value 6.060259\n",
      "iter 880 value 6.052511\n",
      "iter 890 value 6.049162\n",
      "iter 900 value 6.046095\n",
      "iter 910 value 6.044777\n",
      "iter 920 value 6.041973\n",
      "iter 930 value 6.038373\n",
      "iter 940 value 6.033391\n",
      "iter 950 value 6.029506\n",
      "iter 960 value 6.020083\n",
      "iter 970 value 6.015411\n",
      "iter 980 value 6.013650\n",
      "iter 990 value 6.010328\n",
      "iter1000 value 6.007778\n",
      "final  value 6.007778 \n",
      "stopped after 1000 iterations\n",
      "[1] \"Number of neurones:  6 Empirical error =  0.015\"\n",
      "# weights:  309\n",
      "initial  value 477.234333 \n",
      "iter  10 value 80.774178\n",
      "iter  20 value 47.667213\n",
      "iter  30 value 28.678172\n",
      "iter  40 value 18.466296\n",
      "iter  50 value 12.597718\n",
      "iter  60 value 9.260483\n",
      "iter  70 value 7.954071\n",
      "iter  80 value 7.168901\n",
      "iter  90 value 6.401215\n",
      "iter 100 value 5.472361\n",
      "iter 110 value 5.177697\n",
      "iter 120 value 4.894712\n",
      "iter 130 value 4.668651\n",
      "iter 140 value 4.517712\n",
      "iter 150 value 4.337169\n",
      "iter 160 value 4.181632\n",
      "iter 170 value 3.987976\n",
      "iter 180 value 3.852264\n",
      "iter 190 value 3.779319\n",
      "iter 200 value 3.701863\n",
      "iter 210 value 3.655631\n",
      "iter 220 value 3.606907\n",
      "iter 230 value 3.536049\n",
      "iter 240 value 3.467650\n",
      "iter 250 value 3.448979\n",
      "iter 260 value 3.426551\n",
      "iter 270 value 3.410865\n",
      "iter 280 value 3.400437\n",
      "iter 290 value 3.392278\n",
      "iter 300 value 3.385645\n",
      "iter 310 value 3.379173\n",
      "iter 320 value 3.371869\n",
      "iter 330 value 3.364319\n",
      "iter 340 value 3.359215\n",
      "iter 350 value 3.356428\n",
      "iter 360 value 3.353600\n",
      "iter 370 value 3.350552\n",
      "iter 380 value 3.348563\n",
      "iter 390 value 3.347142\n",
      "iter 400 value 3.346058\n",
      "iter 410 value 3.345192\n",
      "iter 420 value 3.338797\n",
      "iter 430 value 3.323086\n",
      "iter 440 value 3.317834\n",
      "iter 450 value 3.314661\n",
      "iter 460 value 3.312259\n",
      "iter 470 value 3.310503\n",
      "iter 480 value 3.309592\n",
      "iter 490 value 3.308745\n",
      "iter 500 value 3.307828\n",
      "iter 510 value 3.306338\n",
      "iter 520 value 3.305134\n",
      "iter 530 value 3.303946\n",
      "iter 540 value 3.302812\n",
      "iter 550 value 3.301734\n",
      "iter 560 value 3.300717\n",
      "iter 570 value 3.298999\n",
      "iter 580 value 3.297240\n",
      "iter 590 value 3.296086\n",
      "iter 600 value 3.295162\n",
      "iter 610 value 3.294750\n",
      "iter 620 value 3.294375\n",
      "iter 630 value 3.294371\n",
      "iter 640 value 3.294352\n",
      "iter 650 value 3.294322\n",
      "iter 660 value 3.294289\n",
      "iter 670 value 3.294248\n",
      "iter 680 value 3.294213\n",
      "iter 690 value 3.294190\n",
      "iter 700 value 3.294166\n",
      "iter 710 value 3.294140\n",
      "iter 720 value 3.294080\n",
      "iter 730 value 3.294011\n",
      "iter 740 value 3.293945\n",
      "iter 750 value 3.293880\n",
      "iter 760 value 3.293793\n",
      "iter 770 value 3.293622\n",
      "iter 780 value 3.293479\n",
      "iter 790 value 3.293081\n",
      "iter 800 value 3.291548\n",
      "iter 810 value 3.289701\n",
      "iter 820 value 3.288997\n",
      "iter 830 value 3.288560\n",
      "iter 840 value 3.288321\n",
      "iter 850 value 3.288149\n",
      "iter 860 value 3.288026\n",
      "iter 870 value 3.287906\n",
      "iter 880 value 3.287789\n",
      "iter 890 value 3.287662\n",
      "iter 900 value 3.287602\n",
      "iter 910 value 3.287518\n",
      "iter 920 value 3.287474\n",
      "iter 930 value 3.287433\n",
      "iter 940 value 3.287412\n",
      "iter 950 value 3.287392\n",
      "iter 960 value 3.287378\n",
      "iter 970 value 3.287367\n",
      "iter 980 value 3.287360\n",
      "iter 990 value 3.287351\n",
      "iter1000 value 3.287340\n",
      "final  value 3.287340 \n",
      "stopped after 1000 iterations\n",
      "[1] \"Number of neurones:  7 Empirical error =  0.0082\"\n",
      "# weights:  353\n",
      "initial  value 632.118864 \n",
      "iter  10 value 106.900760\n",
      "iter  20 value 56.792992\n",
      "iter  30 value 36.879290\n",
      "iter  40 value 26.572477\n",
      "iter  50 value 18.706441\n",
      "iter  60 value 12.754873\n",
      "iter  70 value 9.954439\n",
      "iter  80 value 7.303242\n",
      "iter  90 value 6.239476\n",
      "iter 100 value 5.347459\n",
      "iter 110 value 5.012904\n",
      "iter 120 value 4.874954\n",
      "iter 130 value 4.752463\n",
      "iter 140 value 4.598202\n",
      "iter 150 value 4.394463\n",
      "iter 160 value 4.141000\n",
      "iter 170 value 3.535754\n",
      "iter 180 value 3.204951\n",
      "iter 190 value 3.035408\n",
      "iter 200 value 2.908902\n",
      "iter 210 value 2.760655\n",
      "iter 220 value 2.671241\n",
      "iter 230 value 2.604396\n",
      "iter 240 value 2.519622\n",
      "iter 250 value 2.459178\n",
      "iter 260 value 2.411047\n",
      "iter 270 value 2.379279\n",
      "iter 280 value 2.344617\n",
      "iter 290 value 2.315283\n",
      "iter 300 value 2.286385\n",
      "iter 310 value 2.267744\n",
      "iter 320 value 2.257914\n",
      "iter 330 value 2.250170\n",
      "iter 340 value 2.243859\n",
      "iter 350 value 2.237125\n",
      "iter 360 value 2.229442\n",
      "iter 370 value 2.214351\n",
      "iter 380 value 2.209334\n",
      "iter 390 value 2.205189\n",
      "iter 400 value 2.202857\n",
      "iter 410 value 2.199393\n",
      "iter 420 value 2.197772\n",
      "iter 430 value 2.196392\n",
      "iter 440 value 2.195034\n",
      "iter 450 value 2.193939\n",
      "iter 460 value 2.193331\n",
      "iter 470 value 2.192776\n",
      "iter 480 value 2.192368\n",
      "iter 490 value 2.192002\n",
      "iter 500 value 2.191712\n",
      "iter 510 value 2.191498\n",
      "iter 520 value 2.191265\n",
      "iter 530 value 2.190861\n",
      "iter 540 value 2.190173\n",
      "iter 550 value 2.189490\n",
      "iter 560 value 2.187789\n",
      "iter 570 value 2.183960\n",
      "iter 580 value 2.181247\n",
      "iter 590 value 2.179462\n",
      "iter 600 value 2.178366\n",
      "iter 610 value 2.177462\n",
      "iter 620 value 2.176841\n",
      "iter 630 value 2.176113\n",
      "iter 640 value 2.175502\n",
      "iter 650 value 2.175053\n",
      "iter 660 value 2.174696\n",
      "iter 670 value 2.174369\n",
      "iter 680 value 2.174026\n",
      "iter 690 value 2.173693\n",
      "iter 700 value 2.173275\n",
      "iter 710 value 2.172702\n",
      "iter 720 value 2.172325\n",
      "iter 730 value 2.171903\n",
      "iter 740 value 2.171500\n",
      "iter 750 value 2.171157\n",
      "iter 760 value 2.170748\n",
      "iter 770 value 2.170458\n",
      "iter 780 value 2.170216\n",
      "iter 790 value 2.169973\n",
      "iter 800 value 2.169784\n",
      "iter 810 value 2.169213\n",
      "iter 820 value 2.169203\n",
      "iter 830 value 2.169179\n",
      "iter 840 value 2.169139\n",
      "iter 850 value 2.169099\n",
      "iter 860 value 2.169056\n",
      "iter 870 value 2.168990\n",
      "iter 880 value 2.168943\n",
      "iter 890 value 2.168898\n",
      "iter 900 value 2.168852\n",
      "iter 910 value 2.168806\n",
      "iter 920 value 2.168736\n",
      "iter 930 value 2.168640\n",
      "iter 940 value 2.168534\n",
      "iter 950 value 2.168462\n",
      "iter 960 value 2.168381\n",
      "iter 970 value 2.168278\n",
      "iter 980 value 2.168182\n",
      "iter 990 value 2.168122\n",
      "iter1000 value 2.168030\n",
      "final  value 2.168030 \n",
      "stopped after 1000 iterations\n",
      "[1] \"Number of neurones:  8 Empirical error =  0.0054\"\n",
      "# weights:  397\n",
      "initial  value 505.589377 \n",
      "iter  10 value 79.117370\n",
      "iter  20 value 37.587475\n",
      "iter  30 value 23.485466\n",
      "iter  40 value 13.359670\n",
      "iter  50 value 8.165105\n",
      "iter  60 value 5.389608\n",
      "iter  70 value 4.203699\n",
      "iter  80 value 3.252846\n",
      "iter  90 value 2.656303\n",
      "iter 100 value 2.229509\n",
      "iter 110 value 1.871641\n",
      "iter 120 value 1.625377\n",
      "iter 130 value 1.478128\n",
      "iter 140 value 1.369107\n",
      "iter 150 value 1.304863\n",
      "iter 160 value 1.245829\n",
      "iter 170 value 1.197352\n",
      "iter 180 value 1.155907\n",
      "iter 190 value 1.111821\n",
      "iter 200 value 1.066801\n",
      "iter 210 value 1.015421\n",
      "iter 220 value 0.979315\n",
      "iter 230 value 0.955885\n",
      "iter 240 value 0.936090\n",
      "iter 250 value 0.922556\n",
      "iter 260 value 0.908841\n",
      "iter 270 value 0.898919\n",
      "iter 280 value 0.888564\n",
      "iter 290 value 0.877204\n",
      "iter 300 value 0.868115\n",
      "iter 310 value 0.861214\n",
      "iter 320 value 0.848844\n",
      "iter 330 value 0.835573\n",
      "iter 340 value 0.822714\n",
      "iter 350 value 0.814311\n",
      "iter 360 value 0.807457\n",
      "iter 370 value 0.801704\n",
      "iter 380 value 0.796884\n",
      "iter 390 value 0.789112\n",
      "iter 400 value 0.781199\n",
      "iter 410 value 0.773202\n",
      "iter 420 value 0.764592\n",
      "iter 430 value 0.756356\n",
      "iter 440 value 0.746418\n",
      "iter 450 value 0.732616\n",
      "iter 460 value 0.719470\n",
      "iter 470 value 0.705811\n",
      "iter 480 value 0.692965\n",
      "iter 490 value 0.683700\n",
      "iter 500 value 0.675799\n",
      "iter 510 value 0.670057\n",
      "iter 520 value 0.666161\n",
      "iter 530 value 0.662328\n",
      "iter 540 value 0.658921\n",
      "iter 550 value 0.654816\n",
      "iter 560 value 0.652182\n",
      "iter 570 value 0.650088\n",
      "iter 580 value 0.647735\n",
      "iter 590 value 0.645280\n",
      "iter 600 value 0.642559\n",
      "iter 610 value 0.639848\n",
      "iter 620 value 0.637162\n",
      "iter 630 value 0.634094\n",
      "iter 640 value 0.631042\n",
      "iter 650 value 0.628595\n",
      "iter 660 value 0.626157\n",
      "iter 670 value 0.623381\n",
      "iter 680 value 0.620606\n",
      "iter 690 value 0.617491\n",
      "iter 700 value 0.614012\n",
      "iter 710 value 0.610859\n",
      "iter 720 value 0.607703\n",
      "iter 730 value 0.603271\n",
      "iter 740 value 0.599301\n",
      "iter 750 value 0.596052\n",
      "iter 760 value 0.592864\n",
      "iter 770 value 0.590537\n",
      "iter 780 value 0.587772\n",
      "iter 790 value 0.584857\n",
      "iter 800 value 0.583036\n",
      "iter 810 value 0.582848\n",
      "iter 820 value 0.582598\n",
      "iter 830 value 0.582264\n",
      "iter 840 value 0.581998\n",
      "iter 850 value 0.581682\n",
      "iter 860 value 0.581252\n",
      "iter 870 value 0.580743\n",
      "iter 880 value 0.579893\n",
      "iter 890 value 0.578944\n",
      "iter 900 value 0.577995\n",
      "iter 910 value 0.577150\n",
      "iter 920 value 0.576278\n",
      "iter 930 value 0.575401\n",
      "iter 940 value 0.574784\n",
      "iter 950 value 0.574211\n",
      "iter 960 value 0.573412\n",
      "iter 970 value 0.572199\n",
      "iter 980 value 0.571036\n",
      "iter 990 value 0.569943\n",
      "iter1000 value 0.568912\n",
      "final  value 0.568912 \n",
      "stopped after 1000 iterations\n",
      "[1] \"Number of neurones:  9 Empirical error =  0.0014\"\n",
      "# weights:  441\n",
      "initial  value 929.929202 \n",
      "iter  10 value 55.737453\n",
      "iter  20 value 23.978647\n",
      "iter  30 value 14.465741\n",
      "iter  40 value 10.445879\n",
      "iter  50 value 6.896369\n",
      "iter  60 value 4.639768\n",
      "iter  70 value 3.350363\n",
      "iter  80 value 2.523427\n",
      "iter  90 value 1.987839\n",
      "iter 100 value 1.543505\n",
      "iter 110 value 1.293024\n",
      "iter 120 value 1.063453\n",
      "iter 130 value 0.862789\n",
      "iter 140 value 0.675212\n",
      "iter 150 value 0.550355\n",
      "iter 160 value 0.452688\n",
      "iter 170 value 0.379481\n",
      "iter 180 value 0.325281\n",
      "iter 190 value 0.282242\n",
      "iter 200 value 0.248431\n",
      "iter 210 value 0.219281\n",
      "iter 220 value 0.202957\n",
      "iter 230 value 0.187232\n",
      "iter 240 value 0.172495\n",
      "iter 250 value 0.158403\n",
      "iter 260 value 0.146422\n",
      "iter 270 value 0.136262\n",
      "iter 280 value 0.127262\n",
      "iter 290 value 0.119049\n",
      "iter 300 value 0.113319\n",
      "iter 310 value 0.108082\n",
      "iter 320 value 0.102602\n",
      "iter 330 value 0.096279\n",
      "iter 340 value 0.090475\n",
      "iter 350 value 0.085894\n",
      "iter 360 value 0.081238\n",
      "iter 370 value 0.076572\n",
      "iter 380 value 0.073188\n",
      "iter 390 value 0.070354\n",
      "iter 400 value 0.067977\n",
      "iter 410 value 0.065900\n",
      "iter 420 value 0.063409\n",
      "iter 430 value 0.061870\n",
      "iter 440 value 0.060679\n",
      "iter 450 value 0.059819\n",
      "iter 460 value 0.058489\n",
      "iter 470 value 0.057341\n",
      "iter 480 value 0.056047\n",
      "iter 490 value 0.054497\n",
      "iter 500 value 0.052912\n",
      "iter 510 value 0.050787\n",
      "iter 520 value 0.049049\n",
      "iter 530 value 0.047634\n",
      "iter 540 value 0.045850\n",
      "iter 550 value 0.044090\n",
      "iter 560 value 0.042079\n",
      "iter 570 value 0.040199\n",
      "iter 580 value 0.038484\n",
      "iter 590 value 0.037214\n",
      "iter 600 value 0.035702\n",
      "iter 610 value 0.034251\n",
      "iter 620 value 0.033064\n",
      "iter 630 value 0.032062\n",
      "iter 640 value 0.031247\n",
      "iter 650 value 0.030526\n",
      "iter 660 value 0.029832\n",
      "iter 670 value 0.029220\n",
      "iter 680 value 0.028622\n",
      "iter 690 value 0.028057\n",
      "iter 700 value 0.027297\n",
      "iter 710 value 0.026666\n",
      "iter 720 value 0.026200\n",
      "iter 730 value 0.025581\n",
      "iter 740 value 0.024868\n",
      "iter 750 value 0.024476\n",
      "iter 760 value 0.024253\n",
      "iter 770 value 0.024037\n",
      "iter 780 value 0.023837\n",
      "iter 790 value 0.023634\n",
      "iter 800 value 0.023391\n",
      "iter 810 value 0.023113\n",
      "iter 820 value 0.022858\n",
      "iter 830 value 0.022696\n",
      "iter 840 value 0.022525\n",
      "iter 850 value 0.022346\n",
      "iter 860 value 0.022162\n",
      "iter 870 value 0.022011\n",
      "iter 880 value 0.021792\n",
      "iter 890 value 0.021696\n",
      "iter 900 value 0.021676\n",
      "iter 910 value 0.021650\n",
      "iter 920 value 0.021618\n",
      "iter 930 value 0.021567\n",
      "iter 940 value 0.021505\n",
      "iter 950 value 0.021436\n",
      "iter 960 value 0.021380\n",
      "iter 970 value 0.021315\n",
      "iter 980 value 0.021211\n",
      "iter 990 value 0.021101\n",
      "iter1000 value 0.021006\n",
      "final  value 0.021006 \n",
      "stopped after 1000 iterations\n",
      "[1] \"Number of neurones:  10 Empirical error =  1e-04\"\n",
      "# weights:  485\n",
      "initial  value 548.701898 \n",
      "iter  10 value 58.716853\n",
      "iter  20 value 25.168212\n",
      "iter  30 value 13.636407\n",
      "iter  40 value 7.317404\n",
      "iter  50 value 4.666886\n",
      "iter  60 value 3.366027\n",
      "iter  70 value 2.290069\n",
      "iter  80 value 1.790859\n",
      "iter  90 value 1.544452\n",
      "iter 100 value 1.304173\n",
      "iter 110 value 1.067987\n",
      "iter 120 value 0.864679\n",
      "iter 130 value 0.655254\n",
      "iter 140 value 0.512094\n",
      "iter 150 value 0.420395\n",
      "iter 160 value 0.344712\n",
      "iter 170 value 0.270300\n",
      "iter 180 value 0.221864\n",
      "iter 190 value 0.183678\n",
      "iter 200 value 0.149730\n",
      "iter 210 value 0.125640\n",
      "iter 220 value 0.109027\n",
      "iter 230 value 0.097176\n",
      "iter 240 value 0.088291\n",
      "iter 250 value 0.079704\n",
      "iter 260 value 0.074560\n",
      "iter 270 value 0.070345\n",
      "iter 280 value 0.064620\n",
      "iter 290 value 0.060108\n",
      "iter 300 value 0.054652\n",
      "iter 310 value 0.048263\n",
      "iter 320 value 0.041775\n",
      "iter 330 value 0.035969\n",
      "iter 340 value 0.032537\n",
      "iter 350 value 0.029065\n",
      "iter 360 value 0.026191\n",
      "iter 370 value 0.024111\n",
      "iter 380 value 0.021938\n",
      "iter 390 value 0.020109\n",
      "iter 400 value 0.018689\n",
      "iter 410 value 0.017344\n",
      "iter 420 value 0.015503\n",
      "iter 430 value 0.013975\n",
      "iter 440 value 0.012443\n",
      "iter 450 value 0.010681\n",
      "iter 460 value 0.009492\n",
      "iter 470 value 0.008713\n",
      "iter 480 value 0.007863\n",
      "iter 490 value 0.007089\n",
      "iter 500 value 0.006474\n",
      "iter 510 value 0.005988\n",
      "iter 520 value 0.005455\n",
      "iter 530 value 0.004970\n",
      "iter 540 value 0.004566\n",
      "iter 550 value 0.004235\n",
      "iter 560 value 0.003963\n",
      "iter 570 value 0.003786\n",
      "iter 580 value 0.003636\n",
      "iter 590 value 0.003492\n",
      "iter 600 value 0.003395\n",
      "iter 610 value 0.003267\n",
      "iter 620 value 0.003101\n",
      "iter 630 value 0.002917\n",
      "iter 640 value 0.002738\n",
      "iter 650 value 0.002586\n",
      "iter 660 value 0.002443\n",
      "iter 670 value 0.002288\n",
      "iter 680 value 0.002152\n",
      "iter 690 value 0.002019\n",
      "iter 700 value 0.001933\n",
      "iter 710 value 0.001844\n",
      "iter 720 value 0.001765\n",
      "iter 730 value 0.001665\n",
      "iter 740 value 0.001571\n",
      "iter 750 value 0.001487\n",
      "iter 760 value 0.001411\n",
      "iter 770 value 0.001342\n",
      "iter 780 value 0.001280\n",
      "iter 790 value 0.001230\n",
      "iter 800 value 0.001169\n",
      "iter 810 value 0.001105\n",
      "iter 820 value 0.001050\n",
      "iter 830 value 0.000998\n",
      "iter 840 value 0.000954\n",
      "iter 850 value 0.000920\n",
      "iter 860 value 0.000889\n",
      "iter 870 value 0.000864\n",
      "iter 880 value 0.000834\n",
      "iter 890 value 0.000804\n",
      "iter 900 value 0.000776\n",
      "iter 910 value 0.000755\n",
      "iter 920 value 0.000734\n",
      "iter 930 value 0.000713\n",
      "iter 940 value 0.000692\n",
      "iter 950 value 0.000667\n",
      "iter 960 value 0.000646\n",
      "iter 970 value 0.000622\n",
      "iter 980 value 0.000616\n",
      "iter 990 value 0.000615\n",
      "iter1000 value 0.000612\n",
      "final  value 0.000612 \n",
      "stopped after 1000 iterations\n",
      "[1] \"Number of neurones:  11 Empirical error =  0\"\n",
      "# weights:  529\n",
      "initial  value 888.201315 \n",
      "iter  10 value 73.662161\n",
      "iter  20 value 34.587193\n",
      "iter  30 value 25.654552\n",
      "iter  40 value 12.842885\n",
      "iter  50 value 6.445008\n",
      "iter  60 value 3.648917\n",
      "iter  70 value 2.246585\n",
      "iter  80 value 1.593100\n",
      "iter  90 value 1.153779\n",
      "iter 100 value 0.904463\n",
      "iter 110 value 0.668053\n",
      "iter 120 value 0.549464\n",
      "iter 130 value 0.451402\n",
      "iter 140 value 0.329293\n",
      "iter 150 value 0.251250\n",
      "iter 160 value 0.199907\n",
      "iter 170 value 0.164511\n",
      "iter 180 value 0.136166\n",
      "iter 190 value 0.110130\n",
      "iter 200 value 0.092677\n",
      "iter 210 value 0.078812\n",
      "iter 220 value 0.067394\n",
      "iter 230 value 0.056932\n",
      "iter 240 value 0.048084\n",
      "iter 250 value 0.041865\n",
      "iter 260 value 0.036689\n",
      "iter 270 value 0.033127\n",
      "iter 280 value 0.028585\n",
      "iter 290 value 0.024863\n",
      "iter 300 value 0.020730\n",
      "iter 310 value 0.016689\n",
      "iter 320 value 0.014186\n",
      "iter 330 value 0.011815\n",
      "iter 340 value 0.009852\n",
      "iter 350 value 0.008424\n",
      "iter 360 value 0.007153\n",
      "iter 370 value 0.005825\n",
      "iter 380 value 0.004584\n",
      "iter 390 value 0.003563\n",
      "iter 400 value 0.002992\n",
      "iter 410 value 0.002495\n",
      "iter 420 value 0.002016\n",
      "iter 430 value 0.001690\n",
      "iter 440 value 0.001475\n",
      "iter 450 value 0.001261\n",
      "iter 460 value 0.001085\n",
      "iter 470 value 0.000928\n",
      "iter 480 value 0.000804\n",
      "iter 490 value 0.000721\n",
      "iter 500 value 0.000645\n",
      "iter 510 value 0.000579\n",
      "iter 520 value 0.000508\n",
      "iter 530 value 0.000459\n",
      "iter 540 value 0.000411\n",
      "iter 550 value 0.000354\n",
      "iter 560 value 0.000317\n",
      "iter 570 value 0.000278\n",
      "iter 580 value 0.000245\n",
      "iter 590 value 0.000212\n",
      "iter 600 value 0.000180\n",
      "iter 610 value 0.000145\n",
      "iter 620 value 0.000126\n",
      "iter 630 value 0.000115\n",
      "iter 640 value 0.000103\n",
      "final  value 0.000099 \n",
      "converged\n",
      "[1] \"Number of neurones:  12 Empirical error =  0\"\n",
      "# weights:  573\n",
      "initial  value 1391.076283 \n",
      "iter  10 value 60.172458\n",
      "iter  20 value 23.869970\n",
      "iter  30 value 11.389979\n",
      "iter  40 value 5.213795\n",
      "iter  50 value 2.781025\n",
      "iter  60 value 1.534070\n",
      "iter  70 value 0.982541\n",
      "iter  80 value 0.654242\n",
      "iter  90 value 0.457662\n",
      "iter 100 value 0.325091\n",
      "iter 110 value 0.214902\n",
      "iter 120 value 0.142506\n",
      "iter 130 value 0.091960\n",
      "iter 140 value 0.062264\n",
      "iter 150 value 0.043837\n",
      "iter 160 value 0.028405\n",
      "iter 170 value 0.016514\n",
      "iter 180 value 0.009505\n",
      "iter 190 value 0.005698\n",
      "iter 200 value 0.003209\n",
      "iter 210 value 0.001691\n",
      "iter 220 value 0.000827\n",
      "iter 230 value 0.000428\n",
      "iter 240 value 0.000173\n",
      "final  value 0.000095 \n",
      "converged\n",
      "[1] \"Number of neurones:  13 Empirical error =  0\"\n",
      "# weights:  617\n",
      "initial  value 1217.541135 \n",
      "iter  10 value 60.567299\n",
      "iter  20 value 22.272345\n",
      "iter  30 value 12.468055\n",
      "iter  40 value 5.474752\n",
      "iter  50 value 2.684692\n",
      "iter  60 value 1.322748\n",
      "iter  70 value 0.789153\n",
      "iter  80 value 0.525658\n",
      "iter  90 value 0.348667\n",
      "iter 100 value 0.235773\n",
      "iter 110 value 0.130993\n",
      "iter 120 value 0.066799\n",
      "iter 130 value 0.034367\n",
      "iter 140 value 0.018706\n",
      "iter 150 value 0.010255\n",
      "iter 160 value 0.005726\n",
      "iter 170 value 0.003377\n",
      "iter 180 value 0.002054\n",
      "iter 190 value 0.001265\n",
      "iter 200 value 0.000801\n",
      "iter 210 value 0.000453\n",
      "iter 220 value 0.000228\n",
      "final  value 0.000099 \n",
      "converged\n",
      "[1] \"Number of neurones:  14 Empirical error =  0\"\n",
      "# weights:  661\n",
      "initial  value 625.271196 \n",
      "iter  10 value 54.174085\n",
      "iter  20 value 23.935428\n",
      "iter  30 value 11.287958\n",
      "iter  40 value 6.595969\n",
      "iter  50 value 3.289589\n",
      "iter  60 value 1.672551\n",
      "iter  70 value 0.858704\n",
      "iter  80 value 0.423895\n",
      "iter  90 value 0.211441\n",
      "iter 100 value 0.135276\n",
      "iter 110 value 0.069850\n",
      "iter 120 value 0.039485\n",
      "iter 130 value 0.020675\n",
      "iter 140 value 0.009370\n",
      "iter 150 value 0.004221\n",
      "iter 160 value 0.001888\n",
      "iter 170 value 0.000807\n",
      "iter 180 value 0.000296\n",
      "iter 190 value 0.000102\n",
      "iter 190 value 0.000091\n",
      "iter 190 value 0.000091\n",
      "final  value 0.000091 \n",
      "converged\n",
      "[1] \"Number of neurones:  15 Empirical error =  0\"\n",
      "# weights:  705\n",
      "initial  value 536.737498 \n",
      "iter  10 value 56.155669\n",
      "iter  20 value 20.712883\n",
      "iter  30 value 9.552513\n",
      "iter  40 value 5.822060\n",
      "iter  50 value 3.657205\n",
      "iter  60 value 1.962678\n",
      "iter  70 value 1.076958\n",
      "iter  80 value 0.564658\n",
      "iter  90 value 0.303774\n",
      "iter 100 value 0.179633\n",
      "iter 110 value 0.107529\n",
      "iter 120 value 0.058971\n",
      "iter 130 value 0.032943\n",
      "iter 140 value 0.018132\n",
      "iter 150 value 0.008896\n",
      "iter 160 value 0.003896\n",
      "iter 170 value 0.001645\n",
      "iter 180 value 0.000625\n",
      "iter 190 value 0.000237\n",
      "final  value 0.000092 \n",
      "converged\n",
      "[1] \"Number of neurones:  16 Empirical error =  0\"\n",
      "# weights:  749\n",
      "initial  value 478.476588 \n",
      "iter  10 value 57.234506\n",
      "iter  20 value 24.455009\n",
      "iter  30 value 12.293854\n",
      "iter  40 value 5.336719\n",
      "iter  50 value 2.599581\n",
      "iter  60 value 1.344643\n",
      "iter  70 value 0.601699\n",
      "iter  80 value 0.267793\n",
      "iter  90 value 0.133675\n",
      "iter 100 value 0.069978\n",
      "iter 110 value 0.043154\n",
      "iter 120 value 0.024789\n",
      "iter 130 value 0.013519\n",
      "iter 140 value 0.005884\n",
      "iter 150 value 0.002186\n",
      "iter 160 value 0.000864\n",
      "iter 170 value 0.000351\n",
      "iter 180 value 0.000128\n",
      "final  value 0.000098 \n",
      "converged\n",
      "[1] \"Number of neurones:  17 Empirical error =  0\"\n",
      "# weights:  793\n",
      "initial  value 1391.008677 \n",
      "iter  10 value 190.641440\n",
      "iter  20 value 63.170663\n",
      "iter  30 value 26.541619\n",
      "iter  40 value 14.663523\n",
      "iter  50 value 8.460467\n",
      "iter  60 value 5.730224\n",
      "iter  70 value 4.158716\n",
      "iter  80 value 3.033885\n",
      "iter  90 value 2.260580\n",
      "iter 100 value 1.599233\n",
      "iter 110 value 1.150477\n",
      "iter 120 value 0.818849\n",
      "iter 130 value 0.597080\n",
      "iter 140 value 0.418634\n",
      "iter 150 value 0.281985\n",
      "iter 160 value 0.219934\n",
      "iter 170 value 0.161405\n",
      "iter 180 value 0.117605\n",
      "iter 190 value 0.067670\n",
      "iter 200 value 0.038519\n",
      "iter 210 value 0.029473\n",
      "iter 220 value 0.023176\n",
      "iter 230 value 0.017818\n",
      "iter 240 value 0.014859\n",
      "iter 250 value 0.012068\n",
      "iter 260 value 0.009258\n",
      "iter 270 value 0.007442\n",
      "iter 280 value 0.006115\n",
      "iter 290 value 0.004889\n",
      "iter 300 value 0.003897\n",
      "iter 310 value 0.003305\n",
      "iter 320 value 0.002873\n",
      "iter 330 value 0.002469\n",
      "iter 340 value 0.002171\n",
      "iter 350 value 0.001833\n",
      "iter 360 value 0.001631\n",
      "iter 370 value 0.001343\n",
      "iter 380 value 0.001124\n",
      "iter 390 value 0.000948\n",
      "iter 400 value 0.000783\n",
      "iter 410 value 0.000652\n",
      "iter 420 value 0.000530\n",
      "iter 430 value 0.000347\n",
      "iter 440 value 0.000192\n",
      "final  value 0.000099 \n",
      "converged\n",
      "[1] \"Number of neurones:  18 Empirical error =  0\"\n",
      "# weights:  837\n",
      "initial  value 760.883144 \n",
      "iter  10 value 60.956625\n",
      "iter  20 value 24.155272\n",
      "iter  30 value 11.177547\n",
      "iter  40 value 4.979296\n",
      "iter  50 value 1.961388\n",
      "iter  60 value 0.787699\n",
      "iter  70 value 0.300532\n",
      "iter  80 value 0.124677\n",
      "iter  90 value 0.059356\n",
      "iter 100 value 0.021435\n",
      "iter 110 value 0.006652\n",
      "iter 120 value 0.002193\n",
      "iter 130 value 0.000519\n",
      "iter 140 value 0.000130\n",
      "final  value 0.000096 \n",
      "converged\n",
      "[1] \"Number of neurones:  19 Empirical error =  0\"\n",
      "# weights:  881\n",
      "initial  value 408.337868 \n",
      "iter  10 value 57.663771\n",
      "iter  20 value 16.481263\n",
      "iter  30 value 7.594324\n",
      "iter  40 value 4.076544\n",
      "iter  50 value 2.063037\n",
      "iter  60 value 0.874165\n",
      "iter  70 value 0.375572\n",
      "iter  80 value 0.149104\n",
      "iter  90 value 0.056090\n",
      "iter 100 value 0.020941\n",
      "iter 110 value 0.008525\n",
      "iter 120 value 0.002991\n",
      "iter 130 value 0.000934\n",
      "iter 140 value 0.000243\n",
      "final  value 0.000093 \n",
      "converged\n",
      "[1] \"Number of neurones:  20 Empirical error =  0\"\n",
      "# weights:  925\n",
      "initial  value 1663.082676 \n",
      "iter  10 value 65.445844\n",
      "iter  20 value 25.608391\n",
      "iter  30 value 12.671372\n",
      "iter  40 value 4.587839\n",
      "iter  50 value 1.619358\n",
      "iter  60 value 0.647548\n",
      "iter  70 value 0.238754\n",
      "iter  80 value 0.094250\n",
      "iter  90 value 0.054870\n",
      "iter 100 value 0.025514\n",
      "iter 110 value 0.010601\n",
      "iter 120 value 0.003026\n",
      "iter 130 value 0.000854\n",
      "iter 140 value 0.000202\n",
      "final  value 0.000096 \n",
      "converged\n",
      "[1] \"Number of neurones:  21 Empirical error =  0\"\n",
      "# weights:  969\n",
      "initial  value 589.369382 \n",
      "iter  10 value 70.770673\n",
      "iter  20 value 20.320507\n",
      "iter  30 value 8.421503\n",
      "iter  40 value 4.164976\n",
      "iter  50 value 1.778048\n",
      "iter  60 value 0.737460\n",
      "iter  70 value 0.327484\n",
      "iter  80 value 0.127328\n",
      "iter  90 value 0.054398\n",
      "iter 100 value 0.017663\n",
      "iter 110 value 0.005939\n",
      "iter 120 value 0.001415\n",
      "iter 130 value 0.000511\n",
      "iter 140 value 0.000145\n",
      "final  value 0.000090 \n",
      "converged\n",
      "[1] \"Number of neurones:  22 Empirical error =  0\"\n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in nnet.default(x, y, w, ...): too many (1013) weights\n",
     "output_type": "error",
     "traceback": [
      "Error in nnet.default(x, y, w, ...): too many (1013) weights\nTraceback:\n",
      "1. predict(nnet(SalePrice ~ ., DS, size = i, linout = T, maxit = 1000), \n .     X.scale)   # at line 4 of file <text>",
      "2. nnet(SalePrice ~ ., DS, size = i, linout = T, maxit = 1000)",
      "3. nnet.formula(SalePrice ~ ., DS, size = i, linout = T, maxit = 1000)",
      "4. nnet.default(x, y, w, ...)",
      "5. stop(gettextf(\"too many (%d) weights\", nwts), domain = NA)"
     ]
    }
   ],
   "source": [
    "DS<-cbind(X.scale,SalePrice=Y.scale)\n",
    "for (i in 5:22){\n",
    "    #model.neural[i] <- nnet(SalePrice~.,DS, size = i, linout=T, maxit = 1000)\n",
    "    Y.hat <- predict(nnet(SalePrice~.,DS, size = i, linout=T, maxit = 1000),X.scale)\n",
    "\n",
    "    empirical_error[i] <- mean((Y.hat-Y.scale)^2) \n",
    "    print(paste(\"Number of neurones: \", i, \"Empirical error = \",round(empirical_error[i],digits=4)))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An analysis of the number of neurones is done. The maximum of neurones used is 20 as more return an error (too many weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "empirical_error <- c(0.0143671130358607, 0.015019445363705, 0.00821834907673351, 0.0054200745645435, 0.00142228029226354, 5.25161597874794e-05, 1.52899010549121e-06, 2.47317346679477e-07, 2.37597648006074e-07, 2.47841099771862e-07, 2.26350153783801e-07, 2.30675129791945e-07, 2.44205643742954e-07, 2.4713486169669e-07, 2.40667851762e-07, 2.31459671090645e-07, 2.39175596505192e-07, 2.24902711484879e-07) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM\njIyampqnp6eysrK9vb3Hx8fQ0NDZ2dnh4eHp6enw8PD////QFLu4AAAACXBIWXMAABJ0AAAS\ndAHeZh94AAAThElEQVR4nO3d4ULaSqOG0QnEiAh4/3e7BaqidVM0L5MMrPWj0h6/zpxsnwKT\nSSgvwGhl6gnALRASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAg\nJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkC\nhAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFB\ngJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAk\nCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKE\nBAFCggAhQYCQIEBIECAkCBgZ0mpRSr/+xxDQmJ+X8NuQjmMtj8MOVxkCJlI7pKEMu5eX7VBW\n1xgCJlI7pK7s9o93ZXGNIWAitUN6ey15/jWlkGhM7ZAe3kLqrjEETKRqSP3jal2eXh/uhvOr\nDUKiMVVDel8nLKXbXWMImEjFkF42m9Wq7w9LDsPZjoREa2qGNKshIElIECAkCBASBAgJAiZY\n/r5gt6yQaEzFkFZC4mZVPY/ULa89BEyj6nukzT8uQwoMAZOou9iwKptrDwFTuMVVu19d9wtj\n3F5Iny57gjpuMKTRfwP8WNWQnh/7w8p3Pzxfa4j3/6mSqKliSLvFyVmk8wvhQqIxFUMaSvd0\nXLTbrru/F8JH3iTs6+yERE0VQ+pO1r4317tng/dITKD6DSK/+01siPe/2aoddd3eM5LzSEyg\n7nuk9fbw6Nv3SIkhYCI1l7+XJ6sJC3cR4pbUPY80HM4jdf3j9c4jwRRub2cDTEBIECAkCJgq\nJJeac1OEBAFe2kGAkCBASBBwcxf2wRRu7sI+mMJ8LuxLDAETucXLKKC627uwDybQ1jOSS/aY\nqZYu7HMRObPV0oV9bmvCbDV0YZ8bbTFfDe1sEBLzJSQIaCgk75GYr6ZCsmrHXLUUkvNIzFZb\nIcFMCQkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQE\nAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQ\nIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQ\nEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQB\nQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAg\nQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBAS\nBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFC\nggAhQYCQIEBIECAkCBASBFQN6fmxL3v98HytIWASFUPaLcqH5VWGgIlUDGko3dPm8Gi77spw\njSFgIhVD6srm/fGmdNcYAiZSMaRS/u83sSFgIp6RIKDue6T19vDIeyRuTc3l7+XJqt1id5Uh\nYBp1zyMNh/NIXf/oPBK3xc4GCBASBAgJAoQEAUKCgKo7Gz65xhAwkYohrYTEzar50m7Tnb94\nIjAETKPqe6TN+Y1BiSFgEnUXG1Yn+1avNARMwaodBAgJAoQEAe4iBAHuIgQB87mL0MVna2F+\n3LMBAtxFCAI8I0GAuwhBgLsIQYC7CEGAnQ0QICQIEBIETBWS80jcFCFBgJd2ECAkCBASBLiw\nDwJc2AcB87mwLzEETMRlFBDgwj4I8IwEAS7sgwAX9kGAC/sgwM4GCBASBAgJAoQEAUKCgJEh\n9Rd+KOyIIaABI0O60udGCInGjAxpUc6eWP0tIdGYkSHt+uU/zq3+ipBozOiXdlf5bDAh0Rgh\nQYDlbwgQEgSMDulpf3FE/xSazrdDwOyNDentGqPzdwUaNQTM38iQVqVbv35Zd2WVmtHXIaAB\no0/IHu/DsCmLzHz+HgIakNoiZPmbuxZ7Rjp7V6AxQ0ADvEeCAKt2EDD+PFLvPBLY2QABrpCF\nAFfIQoArZCHAFbIQ4MI+CBASBFj+hgDL3xBg+RsCLH9DgOVvCLBqBwFCggDL3xAgJAgYHdK6\n37+q67eh+Xw3BMxe5FLz1z/roiUJicaMvvnJcrcPaVUeYlN6qRNSeH2E+zYypK7sjrsbWlu1\nu8KkuWeBLUJthlRpHO5EYIvQvqHWbllcvnyFcTLvkZq7QaSQyBq7ate3eYNIIZEVOY/U4A0i\nvUci6l53Nli1IyoZUu7n0nkkGnO/IUGQkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIOBe99pB\n1IiQymcTzwqmJCQI8NIOAoQEAamQnvuxM/nnEDBfY0MavEeC0SF9dLSOTelFSDRn9J1Wn16W\nZbtdlugH9wmJxgTutPr4+my0yd6PS0g0JhDSen9zSO+RuGsjQ+pfX9pty+LlWUjctZEhrfcB\nHT4jqbmPdYGgscvfj/vfPZQyhObzzRAwf3Y2QICQIGBsSLuhe/21G3ah+XwzBMzfyJC23Z+7\n0fswZu7ayJCW5WH/XLQbik2r3LPACdnPDyKERGNG77U7vjnaCYm7Nnr393K/W/V5mT2RJCQa\nM3bVbtnmZ8hC1ujzSE/7z5BdRj/TXEg0xwlZCBASBIy6r92ne9tNPCuYkpAgwEs7CBh7hWz2\nOqTvhoAGpLYIZQmJxowMaVGy1098MwQ0YGRIu34ZvaHdN0NAA0a/tLNqB0KCCMvfECAkCLCz\nAQKEBAFe2kGAkCAgc4XsQ/Tz+oREc1L3bIje1k5ItGb0XYS6/ZPRuivRuzYIicaMvq/d5vB1\nUxaZ+fw9BDTAnVYhYPRLu7dnJPf+5p6N/sS+w3uk584NIrlrud3fyd0NQqIxQoIAOxsgQEgQ\nICQIGBvS0LmMAsafR3I9EgRW7cKfjPT3ENAAd1qFgNEv7dxpFQLXIy23qan83xAwf2NDWlts\ngNEhPVq1g5fAhX1W7cCqHUSMfmln1Q4CF/b5fCTwsS4QISQIcBkFBAgJAkaHtO73r+r67EYh\nIdGYyL2/X/+si5YkJBozMqRVWe72Ia3KQ2xKL0KiOaO3CO2Ouxus2nHXAluEhAQjQ1r8eUby\naRTct8x7JJ+PxJ0bu2rX/9nX4Cb63LXIeaTSP4Wm8+0QMHt2NkCAkCBASBAgJAgQEgQICQLq\nh7RalNKvrzoE1FYxpON2vOXxBO5wlSFgIrVDGsqwe3nZDue3FAmJxtQOqTveCG93fpOrkGhM\n7ZDeLrc4f9nFbEIK3x2Jm1U7pIe3kLprDBF2hSutuFFVQ+ofV+uy39+6G86vNszkZ7ec/Arn\nVA3p/U6SpXRn7xk+j5/d8uUr/K+a55E2m9Wq7w9LDsP5e+/P40dXSFzMzob/JyQuJqQzvEfi\nUlVDen48XpneD//4LJiZ/OxateNSFUPaLU4+u+L8PR5m87PrPBKXqRjSULqnzeHRdt01sfwN\nl6oYUlc27483TZyQhUtV3/393W9iQ8BEPCNBQN33SOvjh794j8Stqbn8vTxZtVs0sEUILlb3\nPNJwOI/U9Y9tnEeCS9nZAAFCggBbhCDAFiEImM8WoXLql0PARJyQhQBbhCDAMxIE2CIEAbYI\nQYAtQhBgZwMECAkChAQBU4XkPBI3RUgQ4KUdBAgJAoQEAS7sgwAX9kHAfC7sSwwBE3EZBQS4\nsA8CPCNBgAv7IMCFfRDgwj4IsLMBAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGA\nkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQI\nEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQE\nAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQ\nIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQ\nEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQB\nQoIAIUGAkCBASAGlNDZh4oQ02qEiKd05IY1WTn7lXglprPLlK3dJSGMJiRchjSckXoQU4D0S\nQgqwaoeQIpxHQkgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQ\nIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQUDWk58e+7PXD87WGgElUDGm3KB+WVxkCJlIx\npKF0T5vDo+26K8M1hoCJVAypK5v3x5vSXWMImEjFkD59ZMP5z28QEo3xjAQBdd8jrbeHR94j\ncWtqLn8vT1btFrurDAHTqHseaTicR+r6R+eRuC12NkCAkCDAFiEIsEUIAmwRggAnZCHAFiEI\n8IwEAbYIQYAtQtWU8y9naZotQpUcKpLSzbKzoZJy8uu5b7uoteB3GTDzVwmpjvLl6/9810VP\nW8HvMmDqr7JFqI4LQ7rge6LfZcDUX2WLUB0XhfST2iLfZcDYX1V3+fvcFqFy6pdDzNgl/+a1\n/GN2+wPOJqT7PiF7yavwln/Mbn/A2YR071uELniibfkdxO0POJf3SPf9jHSRlte0bn/Auaza\n2SJ0gZbPstz+gDM5j3TnW4S4ZTVDuustQty2qiHNaQhIEhIECAkCpgrpDs8jccuEBAFe2kGA\nkCBASBBQ94Ts/V7Yx42rGNJdX9jHjau7adW9v7lRLqOAABf2QYBnJAhwYR8EuLAPAmZ6YR80\n5uc5TPl00fRTVcuTN/c8If1Sy5M39zwh/VLLkzf3PCH9UsuTN/c8If1Sy5M39zwh/VLLkzf3\nPCH9UsuTN/c8If1Sy5M39zwh/VLLkzf3PCH9UsuTN/c8If1Sy5M39zwh/VLLkzf3vLnOC5oi\nJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgYKqQfn2z8umt3mY9\ndKUbzn4Qx+y8zb29w79avB/sWR73iY7lpr3/km82b7M+frDNYtrZ/Mzb3Ns7/MNhvt0+n3ke\n98lC6qcZeLRN9+fn77l0m/3v/vGhNnPyPvfmDv+mPOz2z6cPsz3uE4W0Ko/TDDzWqiz//DAO\nZf3661ND/498zL25w98f572f/kyP+2QhraYZeKwyvH30dF/2H/3Z0r/tH3Nv9fDvpz/T4z5R\nSH1ZP7y+Y5xm8DE275/h/vlLCz7m3ujh35XlbI/7ZCEdLKcZfZxmQ3o5CanJw7/av6qb6XGf\naDqlPL3+AzM0+QrjBkJq8/Bvu/3LuZke90mns5vdIuYlbiCko8YO/647PIPO9LhPO525HY2L\n/Jl0N8//oOd9nm1bc18es5/pcRfSj31atdvObfXovHZD2i6W28ODmR73iQ5lV/bnqGd3NC7y\n58fv8XA+Y12aWvt6fzZt7fCv31dGZnrcJwpp2B+H3fHcWmva3dnwPvfmDv/2Y4Vxpsd9opB2\n3WH9dWb/qlzm7QXRosEl5D9zb+7wP5SP3YHzPO5TvUreDV1ZtLX6+uYtpN1hF/K0c/mp07m3\ndPjLSUjzPO4Nvd2E+RISBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBA\nSBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEFIr\n1vtPTv743Lo3q0Xpht3nh30znw17O4TUiO3+g8g3f4U0HH7f7T493JXtZPO8V0JqxHL/qamb\n0n/+0015eA1nVR4+PXwZ5vZRxbdPSG142j8hvWby+PmP++N/v/1z1MnD16ekp7rTQ0izNnRl\neXiZtjg8x6zKxyeRn77AO3l8fLhc1Jkfb4Q0Z8u3dz3Px4L6sn4o3XD4v53EsyvLLw9X5bnu\nTO+ekGbsqSx3Lw9l2C8kbPZ/0B/XGr6+A1qV9ZeHmzJUnCdCmrV+/7yyK93+qemwwl327312\nw8kLvL1t1399uPsrNq5LSDP28ert06mjXfn0DmjXLf9+WPyHrcvxnrH/CenL704WFj4eCqky\nx3vGLglpu1hu/34opNoc7xlbfn2P1B2+bE9Oy64/3gydPPQeqTohzdhqv2o3HFftDsvZh8e7\n4WORbvsRzPa0nWerdpUJac5OziMdtjTsusPy9yGSw4u3h/K+++7k4cvLo/NIlQlp1oZS+pOd\nDa/PRl1ZHBe/D8WUj3rKaUh2NtQmpDasf7She1tcSFGZkBqx/MmbHru/qxNSI7bHdbuLuB6p\nPiG1Yv1w8bc+eGFXnZAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFC\nggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQI+A8wykqSa+KqmAAAAABJRU5E\nrkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(c(5:22),empirical_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(c(12:22),empirical_error[8:18])  # zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "size.CV<-floor(N/10)\n",
    "\n",
    "CV.err<-numeric(10)\n",
    "\n",
    "for (j in 10:22){\n",
    "    \n",
    "    for (i in 1:10) {\n",
    "        # 1/10 for testing\n",
    "         i.ts<-(((i-1)*size.CV+1):(i*size.CV))  ### i.ts = indices of the test set for the i-th fold\n",
    "         X.ts<-X[i.ts,]  \n",
    "         Y.ts<-Y[i.ts]    \n",
    "\n",
    "        #9/10 for training\n",
    "         i.tr<-setdiff(1:N,i.ts)                ###i.tr = indices of the training set for the i-th fold\n",
    "         X.tr<-X[i.tr,]\n",
    "         Y.tr<-Y[i.tr]         \n",
    "\n",
    "        #scaling\n",
    "         X.tr.mean <- colMeans(X.tr)\n",
    "         X.tr.sd <- apply(X.tr,2,sd)\n",
    "        Y.tr.mean <- mean(Y.tr)\n",
    "        Y.tr.sd <- sd(Y.tr)\n",
    "\n",
    "        Y.tr <- cbind(Y.tr - Y.tr.mean)/Y.tr.sd \n",
    "        X.tr <- t(apply(sweep(X.tr,2,X.tr.mean,\"-\"), 1, function(x) x/X.tr.sd))\n",
    "\n",
    "        #scaling the testing test by the same scaling as of the training set\n",
    "        X.ts <- t(apply(sweep(X.ts,2,X.tr.mean,\"-\"), 1, function(x) x/X.tr.sd))\n",
    "        Y.ts <- cbind(Y.ts - Y.tr.mean)/Y.tr.sd \n",
    "\n",
    "        X.ts <-data.frame(X.ts)\n",
    "        X.tr <-data.frame(X.tr)                       \n",
    "\n",
    "         DS <- cbind(X.tr,SalePrice=Y.tr)\n",
    "        \n",
    "         model.neural <- nnet(SalePrice~.,DS, size = j, linout=T, maxit = 1000)\n",
    "\n",
    "         Y.hat.ts<- predict(model.neural,X.ts)\n",
    "\n",
    "         CV.err[i]<-mean((Y.hat.ts-Y.ts)^2)\n",
    "        }\n",
    "    mean.size[j-9] = round(mean(CV.err),digits=4)\n",
    "    print(paste(\"Number of neurones: \", j, \" CV error = \",mean.size[j-9], \" std dev = \",round(sd(CV.err),digits=4)))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size.CV<-floor(N/10)\n",
    "\n",
    "CV.err<-numeric(10)\n",
    "\n",
    "for (i in 1:10) {\n",
    "    # 1/10 for testing\n",
    "     i.ts<-(((i-1)*size.CV+1):(i*size.CV))  ### i.ts = indices of the test set for the i-th fold\n",
    "     X.ts<-X[i.ts,]  \n",
    "     Y.ts<-Y[i.ts]    \n",
    "    \n",
    "    #9/10 for training\n",
    "     i.tr<-setdiff(1:N,i.ts)                ###i.tr = indices of the training set for the i-th fold\n",
    "     X.tr<-X[i.tr,]\n",
    "     Y.tr<-Y[i.tr]         \n",
    "     \n",
    "    #scaling\n",
    "     X.tr.mean <- colMeans(X.tr)\n",
    "     X.tr.sd <- apply(X.tr,2,sd)\n",
    "    Y.tr.mean <- mean(Y.tr)\n",
    "    Y.tr.sd <- sd(Y.tr)\n",
    "\n",
    "    Y.tr <- cbind(Y.tr - Y.tr.mean)/Y.tr.sd \n",
    "    X.tr <- t(apply(sweep(X.tr,2,X.tr.mean,\"-\"), 1, function(x) x/X.tr.sd))\n",
    "    \n",
    "    #scaling the testing test by the same scaling as of the training set\n",
    "    X.ts <- t(apply(sweep(X.ts,2,X.tr.mean,\"-\"), 1, function(x) x/X.tr.sd))\n",
    "    Y.ts <- cbind(Y.ts - Y.tr.mean)/Y.tr.sd \n",
    "    \n",
    "    X.ts <-data.frame(X.ts)\n",
    "    X.tr <-data.frame(X.tr)\n",
    "        \n",
    "\n",
    "    DS<-cbind(X.tr,SalePrice=Y.tr)\n",
    "    \n",
    "    model.linear<- lm(SalePrice~.,DS)      # create model with the training set\n",
    "        \n",
    "    Y.hat.ts<- predict(model.linear,X.ts)  # predict value for the test set\n",
    "        \n",
    "    CV.err[i]<-mean((Y.hat.ts-Y.ts)^2)  # MSE for test set\n",
    "}\n",
    "    \n",
    "print(paste(\"CV error=\",round(mean(CV.err),digits=4), \" ; std dev=\",round(sd(CV.err),digits=4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble techniques : Combination of models strategy\n",
    "> Methodology and main results\n",
    "\n",
    "> The text should mention the different models taken into consideration as well as the techniques used for the combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion and conclusion: \n",
    "> Summary of your work, and discussion of what worked well, not well, why, what insights you got from the analyses you made. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R 3.3",
   "language": "R",
   "name": "ir33"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
