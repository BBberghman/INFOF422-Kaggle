{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# INFO F422 - Statistical Fundation of Machine Learning\n",
    "## Project \"House Prices : Advanced Regression Techniques\"\n",
    "\n",
    "    Erica Berghman\n",
    "    Master 1 - Brussels Engineer School"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction \n",
    "\n",
    "> with dataset description, goals, and an overview of the report structure \n",
    "\n",
    "> Starting from a data set with 81 criteria about houses and their selling price, the goal is to create a model capable of predicting the price of other houses given some of these criterias. A good model description is a model that has been refined multiple types. This report will show the methodology used to construct a model for this particular problem. It is based on the methodology of the Chapter 6 of the syllabus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataSample = 400\n",
    "mean = T          # variable to determine if we use the mean or the median to replace the NA values\n",
    "set.seed(3)\n",
    "\n",
    "source(\"functions/replaceNA.R\")\n",
    "# Hide warnings\n",
    "options(warn=-1)\n",
    "print = T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get a model, the data must be preprocessed. Firstly we read the data given and we take a sample set of 400 houses out of the 1460. There is 81 criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Id</th><th scope=col>MSSubClass</th><th scope=col>MSZoning</th><th scope=col>LotFrontage</th><th scope=col>LotArea</th><th scope=col>Street</th><th scope=col>Alley</th><th scope=col>LotShape</th><th scope=col>LandContour</th><th scope=col>Utilities</th><th scope=col>...</th><th scope=col>PoolArea</th><th scope=col>PoolQC</th><th scope=col>Fence</th><th scope=col>MiscFeature</th><th scope=col>MiscVal</th><th scope=col>MoSold</th><th scope=col>YrSold</th><th scope=col>SaleType</th><th scope=col>SaleCondition</th><th scope=col>SalePrice</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>1     </td><td>60    </td><td>RL    </td><td>65    </td><td>8450  </td><td>Pave  </td><td>NA    </td><td>Reg   </td><td>Lvl   </td><td>AllPub</td><td>...   </td><td>0     </td><td>NA    </td><td>NA    </td><td>NA    </td><td>0     </td><td>2     </td><td>2008  </td><td>WD    </td><td>Normal</td><td>208500</td></tr>\n",
       "\t<tr><td>2     </td><td>20    </td><td>RL    </td><td>80    </td><td>9600  </td><td>Pave  </td><td>NA    </td><td>Reg   </td><td>Lvl   </td><td>AllPub</td><td>...   </td><td>0     </td><td>NA    </td><td>NA    </td><td>NA    </td><td>0     </td><td>5     </td><td>2007  </td><td>WD    </td><td>Normal</td><td>181500</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll}\n",
       " Id & MSSubClass & MSZoning & LotFrontage & LotArea & Street & Alley & LotShape & LandContour & Utilities & ... & PoolArea & PoolQC & Fence & MiscFeature & MiscVal & MoSold & YrSold & SaleType & SaleCondition & SalePrice\\\\\n",
       "\\hline\n",
       "\t 1      & 60     & RL     & 65     & 8450   & Pave   & NA     & Reg    & Lvl    & AllPub & ...    & 0      & NA     & NA     & NA     & 0      & 2      & 2008   & WD     & Normal & 208500\\\\\n",
       "\t 2      & 20     & RL     & 80     & 9600   & Pave   & NA     & Reg    & Lvl    & AllPub & ...    & 0      & NA     & NA     & NA     & 0      & 5      & 2007   & WD     & Normal & 181500\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "Id | MSSubClass | MSZoning | LotFrontage | LotArea | Street | Alley | LotShape | LandContour | Utilities | ... | PoolArea | PoolQC | Fence | MiscFeature | MiscVal | MoSold | YrSold | SaleType | SaleCondition | SalePrice | \n",
       "|---|---|\n",
       "| 1      | 60     | RL     | 65     | 8450   | Pave   | NA     | Reg    | Lvl    | AllPub | ...    | 0      | NA     | NA     | NA     | 0      | 2      | 2008   | WD     | Normal | 208500 | \n",
       "| 2      | 20     | RL     | 80     | 9600   | Pave   | NA     | Reg    | Lvl    | AllPub | ...    | 0      | NA     | NA     | NA     | 0      | 5      | 2007   | WD     | Normal | 181500 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  Id MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour\n",
       "1 1  60         RL       65          8450    Pave   NA    Reg      Lvl        \n",
       "2 2  20         RL       80          9600    Pave   NA    Reg      Lvl        \n",
       "  Utilities ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold\n",
       "1 AllPub    ... 0        NA     NA    NA          0       2      2008  \n",
       "2 AllPub    ... 0        NA     NA    NA          0       5      2007  \n",
       "  SaleType SaleCondition SalePrice\n",
       "1 WD       Normal        208500   \n",
       "2 WD       Normal        181500   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data<-read.csv(\"input/train.csv\")\n",
    "data.sample<-data[sample(nrow(data),dataSample),]\n",
    "if (print) {\n",
    "    dim(data.sample)\n",
    "    data[1:2,]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Categorical criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical (factor) criterias are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    MSZoning    Street     Alley     LotShape  LandContour  Utilities  \n",
       " C (all):  1   Grvl:  0   Grvl: 12   IR1:130   Bnk: 18     AllPub:399  \n",
       " FV     : 19   Pave:400   Pave: 14   IR2: 14   HLS: 14     NoSeWa:  1  \n",
       " RH     :  7              NA's:374   IR3:  2   Low:  8                 \n",
       " RL     :308                         Reg:254   Lvl:360                 \n",
       " RM     : 65                                                           \n",
       "                                                                       \n",
       "                                                                       \n",
       "   LotConfig   LandSlope  Neighborhood   Condition1    Condition2    BldgType  \n",
       " Corner : 69   Gtl:379   NAmes  : 65   Norm   :338   Norm   :398   1Fam  :329  \n",
       " CulDSac: 29   Mod: 19   CollgCr: 37   Feedr  : 22   Artery :  1   2fmCon: 10  \n",
       " FR2    : 10   Sev:  2   OldTown: 32   Artery : 14   Feedr  :  1   Duplex: 14  \n",
       " FR3    :  0             NWAmes : 30   PosN   :  8   PosA   :  0   Twnhs : 18  \n",
       " Inside :292             Somerst: 26   RRAn   :  8   PosN   :  0   TwnhsE: 29  \n",
       "                         Gilbert: 23   PosA   :  4   RRAe   :  0               \n",
       "                         (Other):187   (Other):  6   (Other):  0               \n",
       "   HouseStyle    RoofStyle      RoofMatl    Exterior1st   Exterior2nd \n",
       " 1Story :188   Flat   :  0   CompShg:396   VinylSd:134   VinylSd:131  \n",
       " 2Story :139   Gable  :322   WdShngl:  3   HdBoard: 73   HdBoard: 67  \n",
       " 1.5Fin : 35   Gambrel:  1   WdShake:  1   MetalSd: 65   MetalSd: 66  \n",
       " SLvl   : 19   Hip    : 74   ClyTile:  0   Wd Sdng: 61   Wd Sdng: 57  \n",
       " SFoyer : 10   Mansard:  3   Membran:  0   Plywood: 18   Plywood: 31  \n",
       " 2.5Unf :  4   Shed   :  0   Metal  :  0   CemntBd: 15   CmentBd: 15  \n",
       " (Other):  5                 (Other):  0   (Other): 34   (Other): 33  \n",
       "   MasVnrType  ExterQual ExterCond  Foundation  BsmtQual   BsmtCond  \n",
       " BrkCmn :  3   Ex:  9    Ex:  0    BrkTil: 46   Ex  : 27   Fa  : 10  \n",
       " BrkFace:134   Fa:  2    Fa:  5    CBlock:178   Fa  : 11   Gd  : 18  \n",
       " None   :227   Gd:129    Gd: 46    PConc :169   Gd  :169   Po  :  1  \n",
       " Stone  : 35   TA:260    Po:  1    Slab  :  4   TA  :188   TA  :366  \n",
       " NA's   :  1             TA:348    Stone :  1   NA's:  5   NA's:  5  \n",
       "                                   Wood  :  2                        \n",
       "                                                                     \n",
       " BsmtExposure BsmtFinType1 BsmtFinType2  Heating    HeatingQC CentralAir\n",
       " Av  : 55     ALQ : 59     ALQ :  7     Floor:  0   Ex:190    N: 27     \n",
       " Gd  : 35     BLQ : 52     BLQ :  8     GasA :388   Fa: 13    Y:373     \n",
       " Mn  : 28     GLQ : 97     GLQ :  4     GasW :  7   Gd: 72              \n",
       " No  :277     LwQ : 29     LwQ : 11     Grav :  2   Po:  0              \n",
       " NA's:  5     Rec : 41     Rec : 20     OthW :  1   TA:125              \n",
       "              Unf :117     Unf :345     Wall :  2                       \n",
       "              NA's:  5     NA's:  5                                     \n",
       " Electrical  KitchenQual Functional FireplaceQu   GarageType  GarageFinish\n",
       " FuseA: 34   Ex: 20      Maj1:  6   Ex  :  3    2Types :  2   Fin : 96    \n",
       " FuseF:  6   Fa: 14      Maj2:  2   Fa  : 11    Attchd :238   RFn :117    \n",
       " FuseP:  0   Gd:164      Min1:  8   Gd  :113    Basment:  7   Unf :162    \n",
       " Mix  :  1   TA:202      Min2:  4   Po  :  4    BuiltIn: 19   NA's: 25    \n",
       " SBrkr:359               Mod :  2   TA  : 89    CarPort:  1               \n",
       "                         Sev :  0   NA's:180    Detchd :108               \n",
       "                         Typ :378               NA's   : 25               \n",
       " GarageQual GarageCond PavedDrive  PoolQC      Fence     MiscFeature\n",
       " Ex  :  1   Ex  :  0   N: 28      Ex  :  0   GdPrv: 20   Gar2:  0   \n",
       " Fa  : 16   Fa  :  6   P:  6      Fa  :  1   GdWo : 18   Othr:  0   \n",
       " Gd  :  5   Gd  :  1   Y:366      Gd  :  0   MnPrv: 40   Shed: 11   \n",
       " Po  :  1   Po  :  1              NA's:399   MnWw :  5   TenC:  1   \n",
       " TA  :352   TA  :367                         NA's :317   NA's:388   \n",
       " NA's: 25   NA's: 25                                                \n",
       "                                                                    \n",
       "    SaleType   SaleCondition\n",
       " WD     :350   Abnorml: 25  \n",
       " New    : 29   AdjLand:  0  \n",
       " COD    : 16   Alloca :  3  \n",
       " ConLI  :  2   Family :  6  \n",
       " ConLD  :  1   Normal :337  \n",
       " ConLw  :  1   Partial: 29  \n",
       " (Other):  1                "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "factor_variables<-which(sapply(data.sample[1,],class)==\"factor\")\n",
    "data.sample.nofactor<-data.sample[,-factor_variables]\n",
    "data.sample.factor<-data.sample[,factor_variables]\n",
    "if (print) summary(data.sample.factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of them are then added. (TODO justification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(dummies)\n",
    "variable_to_keep<-c(\"CentralAir\", \"Street\", \"LotShape\")\n",
    "data_factor_onehot <- dummy.data.frame(data.sample.factor[,variable_to_keep], sep=\"_\")\n",
    "data.nofactor.extended<-cbind(data.sample.nofactor,data_factor_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Missing data \n",
    "The missing values (NA) are replaced by an estimator of these values (eg. mean or median)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if (mean) {\n",
    "    data_preprocessed<-data.frame(apply(data.nofactor.extended,2,replace_na_with_mean_value)) \n",
    "} else {\n",
    "    data_preprocessed<-data.frame(apply(data.nofactor.extended,2,replace_na_with_median_value))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature selection \n",
    "> Methodology and main results\n",
    "\n",
    "> The text must contain the list of selected variables and the motivation of their choice. The use of formulas, tables and pseudo-code to describe the feature selection procedure is encouraged. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Redundant and irrelevant features \n",
    "The \"Id\" column which is irrelevant is deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_preprocessed<-data_preprocessed[,setdiff(colnames(data_preprocessed),\"Id\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The criterias that are redundant (linear combination of others criterias and correlation > 0.99) are deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>MSSubClass</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>LotFrontage</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>LotArea</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>OverallQual</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>OverallCond</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>YearBuilt</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>YearRemodAdd</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>MasVnrArea</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>BsmtFinSF1</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>BsmtFinSF2</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>BsmtUnfSF</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>TotalBsmtSF</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>X1stFlrSF</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>X2ndFlrSF</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>LowQualFinSF</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>GrLivArea</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>BsmtFullBath</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>BsmtHalfBath</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>FullBath</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>HalfBath</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>BedroomAbvGr</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>KitchenAbvGr</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>TotRmsAbvGrd</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>Fireplaces</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>GarageYrBlt</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>GarageCars</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>GarageArea</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>WoodDeckSF</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>OpenPorchSF</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>EnclosedPorch</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>X3SsnPorch</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>ScreenPorch</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>PoolArea</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>MiscVal</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>MoSold</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>YrSold</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>SalePrice</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>CentralAir_N</dt>\n",
       "\t\t<dd>TRUE</dd>\n",
       "\t<dt>CentralAir_Y</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>LotShape_IR1</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>LotShape_IR2</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>LotShape_IR3</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "\t<dt>LotShape_Reg</dt>\n",
       "\t\t<dd>FALSE</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[MSSubClass] FALSE\n",
       "\\item[LotFrontage] FALSE\n",
       "\\item[LotArea] FALSE\n",
       "\\item[OverallQual] FALSE\n",
       "\\item[OverallCond] FALSE\n",
       "\\item[YearBuilt] FALSE\n",
       "\\item[YearRemodAdd] FALSE\n",
       "\\item[MasVnrArea] FALSE\n",
       "\\item[BsmtFinSF1] FALSE\n",
       "\\item[BsmtFinSF2] FALSE\n",
       "\\item[BsmtUnfSF] FALSE\n",
       "\\item[TotalBsmtSF] FALSE\n",
       "\\item[X1stFlrSF] FALSE\n",
       "\\item[X2ndFlrSF] FALSE\n",
       "\\item[LowQualFinSF] FALSE\n",
       "\\item[GrLivArea] FALSE\n",
       "\\item[BsmtFullBath] FALSE\n",
       "\\item[BsmtHalfBath] FALSE\n",
       "\\item[FullBath] FALSE\n",
       "\\item[HalfBath] FALSE\n",
       "\\item[BedroomAbvGr] FALSE\n",
       "\\item[KitchenAbvGr] FALSE\n",
       "\\item[TotRmsAbvGrd] FALSE\n",
       "\\item[Fireplaces] FALSE\n",
       "\\item[GarageYrBlt] FALSE\n",
       "\\item[GarageCars] FALSE\n",
       "\\item[GarageArea] FALSE\n",
       "\\item[WoodDeckSF] FALSE\n",
       "\\item[OpenPorchSF] FALSE\n",
       "\\item[EnclosedPorch] FALSE\n",
       "\\item[X3SsnPorch] FALSE\n",
       "\\item[ScreenPorch] FALSE\n",
       "\\item[PoolArea] FALSE\n",
       "\\item[MiscVal] FALSE\n",
       "\\item[MoSold] FALSE\n",
       "\\item[YrSold] FALSE\n",
       "\\item[SalePrice] FALSE\n",
       "\\item[CentralAir\\textbackslash{}\\_N] TRUE\n",
       "\\item[CentralAir\\textbackslash{}\\_Y] FALSE\n",
       "\\item[LotShape\\textbackslash{}\\_IR1] FALSE\n",
       "\\item[LotShape\\textbackslash{}\\_IR2] FALSE\n",
       "\\item[LotShape\\textbackslash{}\\_IR3] FALSE\n",
       "\\item[LotShape\\textbackslash{}\\_Reg] FALSE\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "MSSubClass\n",
       ":   FALSELotFrontage\n",
       ":   FALSELotArea\n",
       ":   FALSEOverallQual\n",
       ":   FALSEOverallCond\n",
       ":   FALSEYearBuilt\n",
       ":   FALSEYearRemodAdd\n",
       ":   FALSEMasVnrArea\n",
       ":   FALSEBsmtFinSF1\n",
       ":   FALSEBsmtFinSF2\n",
       ":   FALSEBsmtUnfSF\n",
       ":   FALSETotalBsmtSF\n",
       ":   FALSEX1stFlrSF\n",
       ":   FALSEX2ndFlrSF\n",
       ":   FALSELowQualFinSF\n",
       ":   FALSEGrLivArea\n",
       ":   FALSEBsmtFullBath\n",
       ":   FALSEBsmtHalfBath\n",
       ":   FALSEFullBath\n",
       ":   FALSEHalfBath\n",
       ":   FALSEBedroomAbvGr\n",
       ":   FALSEKitchenAbvGr\n",
       ":   FALSETotRmsAbvGrd\n",
       ":   FALSEFireplaces\n",
       ":   FALSEGarageYrBlt\n",
       ":   FALSEGarageCars\n",
       ":   FALSEGarageArea\n",
       ":   FALSEWoodDeckSF\n",
       ":   FALSEOpenPorchSF\n",
       ":   FALSEEnclosedPorch\n",
       ":   FALSEX3SsnPorch\n",
       ":   FALSEScreenPorch\n",
       ":   FALSEPoolArea\n",
       ":   FALSEMiscVal\n",
       ":   FALSEMoSold\n",
       ":   FALSEYrSold\n",
       ":   FALSESalePrice\n",
       ":   FALSECentralAir_N\n",
       ":   TRUECentralAir_Y\n",
       ":   FALSELotShape_IR1\n",
       ":   FALSELotShape_IR2\n",
       ":   FALSELotShape_IR3\n",
       ":   FALSELotShape_Reg\n",
       ":   FALSE\n",
       "\n"
      ],
      "text/plain": [
       "   MSSubClass   LotFrontage       LotArea   OverallQual   OverallCond \n",
       "        FALSE         FALSE         FALSE         FALSE         FALSE \n",
       "    YearBuilt  YearRemodAdd    MasVnrArea    BsmtFinSF1    BsmtFinSF2 \n",
       "        FALSE         FALSE         FALSE         FALSE         FALSE \n",
       "    BsmtUnfSF   TotalBsmtSF     X1stFlrSF     X2ndFlrSF  LowQualFinSF \n",
       "        FALSE         FALSE         FALSE         FALSE         FALSE \n",
       "    GrLivArea  BsmtFullBath  BsmtHalfBath      FullBath      HalfBath \n",
       "        FALSE         FALSE         FALSE         FALSE         FALSE \n",
       " BedroomAbvGr  KitchenAbvGr  TotRmsAbvGrd    Fireplaces   GarageYrBlt \n",
       "        FALSE         FALSE         FALSE         FALSE         FALSE \n",
       "   GarageCars    GarageArea    WoodDeckSF   OpenPorchSF EnclosedPorch \n",
       "        FALSE         FALSE         FALSE         FALSE         FALSE \n",
       "   X3SsnPorch   ScreenPorch      PoolArea       MiscVal        MoSold \n",
       "        FALSE         FALSE         FALSE         FALSE         FALSE \n",
       "       YrSold     SalePrice  CentralAir_N  CentralAir_Y  LotShape_IR1 \n",
       "        FALSE         FALSE          TRUE         FALSE         FALSE \n",
       " LotShape_IR2  LotShape_IR3  LotShape_Reg \n",
       "        FALSE         FALSE         FALSE "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>400</li>\n",
       "\t<li>42</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 400\n",
       "\\item 42\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 400\n",
       "2. 42\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 400  42"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#library(caret)\n",
    "library(ggplot2)\n",
    "library(lattice)\n",
    "\n",
    "#linearCombo.idx <- findLinearCombos(data_preprocessed)$remove\n",
    "#if (!is.null(linearCombo.idx)) data_preprocessed<-data_preprocessed[,-linearCombo.idx]\n",
    "\n",
    "correlation.matrix <- cor(data_preprocessed)\n",
    "correlation.matrix[upper.tri(correlation.matrix)] <- 0\n",
    "diag(correlation.matrix) <- 0\n",
    "data.uncorrelated <- data_preprocessed[,!apply(correlation.matrix,2,function(x) any(abs(x) > 0.99))]\n",
    "if (print) dim(data.uncorrelated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input and output vectors are created and scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X <- data.uncorrelated[,setdiff(colnames(data.uncorrelated),\"SalePrice\")]\n",
    "Y <- data.uncorrelated[,\"SalePrice\"]\n",
    "X <- data.frame(X)\n",
    "#Y <- data.frame(Y)\n",
    "X.scale <- data.frame(scale(X))\n",
    "Y.scale <- scale(Y)\n",
    "\n",
    "N<-nrow(X)    #Number of examples\n",
    "n<-ncol(X)    #Number of input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>400</li>\n",
       "\t<li>41</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 400\n",
       "\\item 41\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 400\n",
       "2. 41\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 400  41"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if (print) dim(X)\n",
    "#X.tr.mean <- colMeans(X)\n",
    "#X.tr.std <- apply(X,2,sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Filter methods\n",
    "\n",
    "   It create a subset of features, removing from the whole features set the ones less likely to determine the variable (SalePrice). It is robust to overfitting and effective in computational time. However it might select redundant variables as the interraction between the variables is not taken in consideration.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source(\"functions/filtre.R\")\n",
    "features.filtre <- filtre(X.scale,Y.scale)  # return the idx of the more correlated features where #feature = argmin(CV error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.filtre = X.scale[,features.filtre]\n",
    "if (print) {\n",
    "    dim(X.filtre)\n",
    "    X.filtre[1:2,]\n",
    "}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source(\"functions/mRMR.R\")\n",
    "features.mrmr <- mrmr(X.scale, Y.scale)    # return the idx of the more correlated features where #feature = argmin(CV error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.mrmr <- X.scale[,features.mrmr]\n",
    "if (print) dim(X.mrmr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source(\"functions/pca.R\")\n",
    "X.pca <- pca(X.scale, Y.scale)   # return X_pca with nb of columns = argmin(CV error)\n",
    "if (print) dim(X.pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Wrapper method\n",
    "\n",
    "Its a cyclic method where a subset of variable is created and evaluated by the Learning Algorithm, modifying the chosen subset. This is done until the best subset is generated.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source(\"functions/wrapper.R\")\n",
    "features.wrapper <- wrapper(X.scale, Y.scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.wrapper <- X[,features.wrapper]\n",
    "if (print) dim(X.wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Hybrid method\n",
    "The filter method is used to select a first \"big\" set of features, that is then refined by the wrapper method. This gives us the possibility use advantages of both method to get a good subset in a relatively correct computational time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features.wrapper.pca <- wrapper(X.pca, Y.scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model selection  \n",
    "> Methodology and main results\n",
    "\n",
    "> For the learning method, the only packages that may be used are those seen during the exercise classes : stats, nnet, tree, lazy, and e1071, for linear models, neural networks, decision trees, nearest neighbours and SVM, respectively.\n",
    "\n",
    "> The accuracy of the regression models during the selection process should be assessed by using the root mean squared error between the logarithm of the predicted value and the logarithm of the observed sale price.\n",
    "\n",
    "> The text must mention the different (and at least three) models which have been taken into consideration and the procedure used for model assessment and selection. The use of formulas, tables and pseudo-code to describe the feature selection procedure is encouraged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Empirical error= 0.1201\"\n"
     ]
    }
   ],
   "source": [
    "DS<-cbind(X.scale,SalePrice=Y.scale)\n",
    "model.linear <- lm(SalePrice~.,DS) ### IMDB score given all the other ones (~.) over the dataset DS\n",
    "\n",
    "Y.hat <- predict(model.linear, X.scale)\n",
    "\n",
    "empirical_error<-mean((Y.hat-Y.scale)^2) ### MSE for prediction of that model.\n",
    "if (print) print(paste(\"Empirical error=\",round(empirical_error,digits=4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"CV error= 0.1645  ; std dev= 0.073\"\n"
     ]
    }
   ],
   "source": [
    "size.CV<-floor(N/10)\n",
    "\n",
    "CV.err<-numeric(10)\n",
    "\n",
    "for (i in 1:10) {\n",
    "    # 1/10 for testing\n",
    "     i.ts<-(((i-1)*size.CV+1):(i*size.CV))  ### i.ts = indices of the test set for the i-th fold\n",
    "     X.ts<-X[i.ts,]  \n",
    "     Y.ts<-Y[i.ts]    \n",
    "    \n",
    "    #9/10 for training\n",
    "     i.tr<-setdiff(1:N,i.ts)                ###i.tr = indices of the training set for the i-th fold\n",
    "     X.tr<-X[i.tr,]\n",
    "     Y.tr<-Y[i.tr]         \n",
    "     \n",
    "    #scaling\n",
    "     X.tr.mean <- colMeans(X.tr)\n",
    "     X.tr.sd <- apply(X.tr,2,sd)\n",
    "    Y.tr.mean <- mean(Y.tr)\n",
    "    Y.tr.sd <- sd(Y.tr)\n",
    "    Y.tr <- cbind(Y.tr - Y.tr.mean)/Y.tr.sd \n",
    "    X.tr <- t(apply(sweep(X.tr,2,X.tr.mean,\"-\"), 1, function(x) x/X.tr.sd))\n",
    "    \n",
    "    #scaling the testing test by the same scaling as of the training set\n",
    "    X.ts <- t(apply(sweep(X.ts,2,X.tr.mean,\"-\"), 1, function(x) x/X.tr.sd))\n",
    "    Y.ts <- cbind(Y.ts - Y.tr.mean)/Y.tr.sd \n",
    "    \n",
    "    X.ts <-data.frame(X.ts)\n",
    "    X.tr <-data.frame(X.tr)\n",
    "        \n",
    "    DS<-cbind(X.tr,SalePrice=Y.tr)\n",
    "  \n",
    "    # delete column with only NAs (due to dividing by a std of 0)    \n",
    "    DS<- DS[,colSums(!is.na(DS)) > 0]    \n",
    "\n",
    "    model.linear<- lm(SalePrice~.,DS)      # create model with the training set\n",
    "    Y.hat.ts<- predict(model.linear,X.ts)  # predict value for the test set\n",
    "        \n",
    "    CV.err[i]<-mean((Y.hat.ts-Y.ts)^2)  # MSE for test set\n",
    "}\n",
    "    \n",
    "print(paste(\"CV error=\",round(mean(CV.err),digits=4), \" ; std dev=\",round(sd(CV.err),digits=4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(rpart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Empirical error= 0.1793\"\n"
     ]
    }
   ],
   "source": [
    "DS<-cbind(X.scale,SalePrice=Y.scale)\n",
    "model.tree <- rpart(SalePrice~.,DS)\n",
    "        \n",
    "Y.hat <- predict(model.tree,X.scale)\n",
    "        \n",
    "empirical_error<-mean((Y.hat-Y.scale)^2) \n",
    "print(paste(\"Empirical error=\",round(empirical_error,digits=4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nRK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(rpart.plot)\n",
    "prp(model.tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Decision tree gives a good overview on what criteria are important. It wears its name well as it help you decide the value of the price given a set of caracteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"CV error = 0.1934  std dev = 0.0577\"\n"
     ]
    }
   ],
   "source": [
    "size.CV<-floor(N/10)\n",
    "\n",
    "CV.err<-numeric(10)\n",
    "\n",
    "for (i in 1:10) {\n",
    "    # 1/10 for testing\n",
    "     i.ts<-(((i-1)*size.CV+1):(i*size.CV))  ### i.ts = indices of the test set for the i-th fold\n",
    "     X.ts<-X[i.ts,]  \n",
    "     Y.ts<-Y[i.ts]    \n",
    "    \n",
    "    #9/10 for training\n",
    "     i.tr<-setdiff(1:N,i.ts)                ###i.tr = indices of the training set for the i-th fold\n",
    "     X.tr<-X[i.tr,]\n",
    "     Y.tr<-Y[i.tr]         \n",
    "     \n",
    "    #scaling\n",
    "     X.tr.mean <- colMeans(X.tr)\n",
    "     X.tr.sd <- apply(X.tr,2,sd)\n",
    "    Y.tr.mean <- mean(Y.tr)\n",
    "    Y.tr.sd <- sd(Y.tr)\n",
    "\n",
    "    Y.tr <- cbind(Y.tr - Y.tr.mean)/Y.tr.sd \n",
    "    X.tr <- t(apply(sweep(X.tr,2,X.tr.mean,\"-\"), 1, function(x) x/X.tr.sd))\n",
    "    \n",
    "    #scaling the testing test by the same scaling as of the training set\n",
    "    X.ts <- t(apply(sweep(X.ts,2,X.tr.mean,\"-\"), 1, function(x) x/X.tr.sd))\n",
    "    Y.ts <- cbind(Y.ts - Y.tr.mean)/Y.tr.sd \n",
    "    \n",
    "    X.ts <-data.frame(X.ts)\n",
    "    X.tr <-data.frame(X.tr)                       \n",
    "     \n",
    "    DS<-cbind(X.tr,SalePrice=Y.tr)\n",
    "    DS<- DS[,colSums(!is.na(DS)) > 0]    \n",
    "        \n",
    "    model<- rpart(SalePrice~.,DS)\n",
    "        \n",
    "    Y.hat.ts<- predict(model.tree,X.ts)\n",
    "        \n",
    "    CV.err[i]<-mean((Y.hat.ts-Y.ts)^2)\n",
    "}\n",
    "    \n",
    "\n",
    "print(paste(\"CV error =\",round(mean(CV.err),digits=4), \" std dev =\",round(sd(CV.err),digits=4)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(nnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# weights:  216\n",
      "initial  value 405.346557 \n",
      "iter  10 value 71.807642\n",
      "iter  20 value 37.024349\n",
      "iter  30 value 22.612993\n",
      "iter  40 value 16.651721\n",
      "iter  50 value 14.215580\n",
      "iter  60 value 12.941675\n",
      "iter  70 value 12.042986\n",
      "iter  80 value 11.384322\n",
      "iter  90 value 10.966827\n",
      "iter 100 value 10.220278\n",
      "iter 110 value 9.968853\n",
      "iter 120 value 9.878128\n",
      "iter 130 value 9.700413\n",
      "iter 140 value 9.368363\n",
      "iter 150 value 9.015004\n",
      "iter 160 value 8.690616\n",
      "iter 170 value 8.450985\n",
      "iter 180 value 8.293419\n",
      "iter 190 value 8.025176\n",
      "iter 200 value 7.841972\n",
      "iter 210 value 7.601023\n",
      "iter 220 value 7.382145\n",
      "iter 230 value 7.259502\n",
      "iter 240 value 7.153716\n",
      "iter 250 value 7.069991\n",
      "iter 260 value 6.980652\n",
      "iter 270 value 6.933445\n",
      "iter 280 value 6.872346\n",
      "iter 290 value 6.791391\n",
      "iter 300 value 6.706091\n",
      "iter 310 value 6.660776\n",
      "iter 320 value 6.596123\n",
      "iter 330 value 6.541293\n",
      "iter 340 value 6.464066\n",
      "iter 350 value 6.396614\n",
      "iter 360 value 6.358123\n",
      "iter 370 value 6.326998\n",
      "iter 380 value 6.295035\n",
      "iter 390 value 6.268988\n",
      "iter 400 value 6.246048\n",
      "iter 410 value 6.220871\n",
      "iter 420 value 6.195125\n",
      "iter 430 value 6.169072\n",
      "iter 440 value 6.158951\n",
      "iter 450 value 6.157385\n",
      "iter 460 value 6.155269\n",
      "iter 470 value 6.153030\n",
      "iter 480 value 6.150495\n",
      "iter 490 value 6.147147\n",
      "iter 500 value 6.143984\n",
      "iter 510 value 6.140085\n",
      "iter 520 value 6.135847\n",
      "iter 530 value 6.129005\n",
      "iter 540 value 6.120384\n",
      "iter 550 value 6.110585\n",
      "iter 560 value 6.097314\n",
      "iter 570 value 6.086227\n",
      "iter 580 value 6.068981\n",
      "iter 590 value 6.058936\n",
      "iter 600 value 6.051207\n",
      "iter 610 value 6.047289\n",
      "iter 620 value 6.041248\n",
      "iter 630 value 6.032757\n",
      "iter 640 value 6.028450\n",
      "iter 650 value 6.025016\n",
      "iter 660 value 6.021874\n",
      "iter 670 value 6.017027\n",
      "iter 680 value 6.013049\n",
      "iter 690 value 6.007263\n",
      "iter 700 value 6.000900\n",
      "iter 710 value 5.992349\n",
      "iter 720 value 5.981681\n",
      "iter 730 value 5.958483\n",
      "iter 740 value 5.942674\n",
      "iter 750 value 5.928886\n",
      "iter 760 value 5.920220\n",
      "iter 770 value 5.917677\n",
      "iter 780 value 5.913963\n",
      "iter 790 value 5.908390\n",
      "iter 800 value 5.901485\n",
      "iter 810 value 5.815367\n",
      "iter 820 value 5.781529\n",
      "iter 830 value 5.774217\n",
      "iter 840 value 5.768825\n",
      "iter 850 value 5.765996\n",
      "iter 860 value 5.759854\n",
      "iter 870 value 5.754316\n",
      "iter 880 value 5.754205\n",
      "iter 890 value 5.754162\n",
      "iter 900 value 5.754053\n",
      "iter 910 value 5.753923\n",
      "iter 920 value 5.753747\n",
      "iter 930 value 5.753514\n",
      "iter 940 value 5.753224\n",
      "iter 950 value 5.752992\n",
      "iter 960 value 5.752633\n",
      "iter 970 value 5.752210\n",
      "iter 980 value 5.751752\n",
      "iter 990 value 5.751262\n",
      "iter1000 value 5.750877\n",
      "final  value 5.750877 \n",
      "stopped after 1000 iterations\n",
      "[1] \"Number of neurones:  5 Empirical error =  0.0144\"\n",
      "# weights:  259\n",
      "initial  value 604.822441 \n",
      "iter  10 value 101.396709\n",
      "iter  20 value 46.973102\n",
      "iter  30 value 27.566774\n",
      "iter  40 value 19.747901\n",
      "iter  50 value 15.817773\n",
      "iter  60 value 14.063874\n",
      "iter  70 value 11.927811\n",
      "iter  80 value 10.685158\n",
      "iter  90 value 9.393779\n",
      "iter 100 value 7.876052\n",
      "iter 110 value 7.021526\n",
      "iter 120 value 6.453036\n",
      "iter 130 value 6.120197\n",
      "iter 140 value 5.894644\n",
      "iter 150 value 5.778385\n",
      "iter 160 value 5.694766\n",
      "iter 170 value 5.579271\n",
      "iter 180 value 5.534541\n",
      "iter 190 value 5.489085\n",
      "iter 200 value 5.452274\n",
      "iter 210 value 5.426705\n",
      "iter 220 value 5.394957\n",
      "iter 230 value 5.375069\n",
      "iter 240 value 5.362802\n",
      "iter 250 value 5.355049\n",
      "iter 260 value 5.346213\n",
      "iter 270 value 5.340578\n",
      "iter 280 value 5.335932\n",
      "iter 290 value 5.326600\n",
      "iter 300 value 5.312618\n",
      "iter 310 value 5.307479\n",
      "iter 320 value 5.301882\n",
      "iter 330 value 5.298060\n",
      "iter 340 value 5.295809\n",
      "iter 350 value 5.294266\n",
      "iter 360 value 5.292300\n",
      "iter 370 value 5.290817\n",
      "iter 380 value 5.289593\n",
      "iter 390 value 5.288748\n",
      "iter 400 value 5.288064\n",
      "iter 410 value 5.287280\n",
      "iter 420 value 5.286774\n",
      "iter 430 value 5.286352\n",
      "iter 440 value 5.285884\n",
      "iter 450 value 5.285277\n",
      "iter 460 value 5.284709\n",
      "iter 470 value 5.284242\n",
      "iter 480 value 5.283662\n",
      "iter 490 value 5.282838\n",
      "iter 500 value 5.281558\n",
      "iter 510 value 5.276716\n",
      "iter 520 value 5.269149\n",
      "iter 530 value 5.269037\n",
      "iter 540 value 5.268788\n",
      "iter 550 value 5.268312\n",
      "iter 560 value 5.267677\n",
      "iter 570 value 5.266728\n",
      "iter 580 value 5.263794\n",
      "iter 590 value 5.258687\n",
      "iter 600 value 5.253669\n",
      "iter 610 value 5.249726\n",
      "iter 620 value 5.247925\n",
      "iter 630 value 5.246925\n",
      "iter 640 value 5.246302\n",
      "iter 650 value 5.245900\n",
      "iter 660 value 5.245578\n",
      "iter 670 value 5.245159\n",
      "iter 680 value 5.243793\n",
      "iter 690 value 5.243742\n",
      "iter 700 value 5.243562\n",
      "iter 710 value 5.243084\n",
      "iter 720 value 5.242744\n",
      "iter 730 value 5.241470\n",
      "iter 740 value 5.223445\n",
      "iter 750 value 5.209836\n",
      "iter 760 value 5.204999\n",
      "iter 770 value 5.203202\n",
      "iter 780 value 5.202400\n",
      "iter 790 value 5.202103\n",
      "iter 800 value 5.201894\n",
      "iter 810 value 5.201549\n",
      "iter 820 value 5.201386\n",
      "iter 830 value 5.201249\n",
      "iter 840 value 5.201114\n",
      "iter 850 value 5.200948\n",
      "iter 860 value 5.200811\n",
      "iter 870 value 5.200632\n",
      "iter 880 value 5.200131\n",
      "iter 890 value 5.199014\n",
      "iter 900 value 5.198504\n",
      "iter 910 value 5.198296\n",
      "iter 920 value 5.198101\n",
      "iter 930 value 5.197910\n",
      "iter 940 value 5.197778\n",
      "iter 950 value 5.197667\n",
      "iter 960 value 5.197571\n",
      "iter 970 value 5.197481\n",
      "iter 980 value 5.197382\n",
      "iter 990 value 5.197217\n",
      "iter1000 value 5.197135\n",
      "final  value 5.197135 \n",
      "stopped after 1000 iterations\n",
      "[1] \"Number of neurones:  6 Empirical error =  0.013\"\n",
      "# weights:  302\n",
      "initial  value 625.292734 \n",
      "iter  10 value 76.070535\n",
      "iter  20 value 34.978795\n",
      "iter  30 value 23.278806\n",
      "iter  40 value 16.113001\n",
      "iter  50 value 11.980166\n",
      "iter  60 value 10.145798\n",
      "iter  70 value 8.125799\n",
      "iter  80 value 7.398216\n",
      "iter  90 value 6.660818\n",
      "iter 100 value 5.831402\n",
      "iter 110 value 5.270837\n",
      "iter 120 value 5.034238\n",
      "iter 130 value 4.866789\n",
      "iter 140 value 4.722274\n",
      "iter 150 value 4.560407\n",
      "iter 160 value 4.383928\n",
      "iter 170 value 4.246410\n",
      "iter 180 value 4.111528\n",
      "iter 190 value 4.024589\n",
      "iter 200 value 3.898026\n",
      "iter 210 value 3.817150\n",
      "iter 220 value 3.748009\n",
      "iter 230 value 3.674282\n",
      "iter 240 value 3.653028\n",
      "iter 250 value 3.628793\n",
      "iter 260 value 3.450677\n",
      "iter 270 value 3.356788\n",
      "iter 280 value 3.343253\n",
      "iter 290 value 3.327622\n",
      "iter 300 value 3.311393\n",
      "iter 310 value 3.292638\n",
      "iter 320 value 3.275003\n",
      "iter 330 value 3.257868\n",
      "iter 340 value 3.231212\n",
      "iter 350 value 3.201849\n",
      "iter 360 value 3.174132\n",
      "iter 370 value 3.142903\n",
      "iter 380 value 3.125639\n",
      "iter 390 value 3.092789\n",
      "iter 400 value 3.068808\n",
      "iter 410 value 3.033441\n",
      "iter 420 value 3.014109\n",
      "iter 430 value 2.995028\n",
      "iter 440 value 2.978723\n",
      "iter 450 value 2.960231\n",
      "iter 460 value 2.945328\n",
      "iter 470 value 2.932830\n",
      "iter 480 value 2.921712\n",
      "iter 490 value 2.913072\n",
      "iter 500 value 2.904627\n",
      "iter 510 value 2.899210\n",
      "iter 520 value 2.895463\n",
      "iter 530 value 2.892202\n",
      "iter 540 value 2.888225\n",
      "iter 550 value 2.884256\n",
      "iter 560 value 2.882481\n",
      "iter 570 value 2.879982\n",
      "iter 580 value 2.876940\n",
      "iter 590 value 2.872777\n",
      "iter 600 value 2.869546\n",
      "iter 610 value 2.866760\n",
      "iter 620 value 2.864800\n",
      "iter 630 value 2.863234\n",
      "iter 640 value 2.860164\n",
      "iter 650 value 2.859806\n",
      "iter 660 value 2.858860\n",
      "iter 670 value 2.856823\n",
      "iter 680 value 2.832873\n",
      "iter 690 value 2.813506\n",
      "iter 700 value 2.807951\n",
      "iter 710 value 2.805293\n",
      "iter 720 value 2.802401\n",
      "iter 730 value 2.799523\n",
      "iter 740 value 2.796979\n",
      "iter 750 value 2.795781\n",
      "iter 760 value 2.794586\n",
      "iter 770 value 2.792813\n",
      "iter 780 value 2.792276\n",
      "iter 790 value 2.791673\n",
      "iter 800 value 2.791187\n",
      "iter 810 value 2.790927\n",
      "iter 820 value 2.790502\n",
      "iter 830 value 2.790292\n",
      "iter 840 value 2.790092\n",
      "iter 850 value 2.789672\n",
      "iter 860 value 2.787107\n",
      "iter 870 value 2.785116\n",
      "iter 880 value 2.784670\n",
      "iter 890 value 2.784516\n",
      "iter 900 value 2.784458\n",
      "iter 910 value 2.784406\n",
      "iter 920 value 2.784312\n",
      "iter 930 value 2.784137\n",
      "iter 940 value 2.783865\n",
      "iter 950 value 2.783666\n",
      "iter 960 value 2.783605\n",
      "iter 970 value 2.783582\n",
      "iter 980 value 2.783539\n",
      "iter 990 value 2.783476\n",
      "iter1000 value 2.783416\n",
      "final  value 2.783416 \n",
      "stopped after 1000 iterations\n",
      "[1] \"Number of neurones:  7 Empirical error =  0.007\"\n",
      "# weights:  345\n",
      "initial  value 601.746353 \n",
      "iter  10 value 88.444362\n",
      "iter  20 value 46.165828\n",
      "iter  30 value 28.739824\n",
      "iter  40 value 20.175998\n",
      "iter  50 value 14.930168\n",
      "iter  60 value 10.735358\n",
      "iter  70 value 8.093673\n",
      "iter  80 value 6.312582\n",
      "iter  90 value 5.432291\n",
      "iter 100 value 4.771241\n",
      "iter 110 value 4.025602\n",
      "iter 120 value 3.597574\n",
      "iter 130 value 3.368115\n",
      "iter 140 value 3.135387\n",
      "iter 150 value 2.839986\n",
      "iter 160 value 2.509872\n",
      "iter 170 value 2.260476\n",
      "iter 180 value 2.096708\n",
      "iter 190 value 2.006909\n",
      "iter 200 value 1.891729\n",
      "iter 210 value 1.819495\n",
      "iter 220 value 1.780657\n",
      "iter 230 value 1.735228\n",
      "iter 240 value 1.699884\n",
      "iter 250 value 1.676584\n",
      "iter 260 value 1.646927\n",
      "iter 270 value 1.620089\n",
      "iter 280 value 1.593611\n",
      "iter 290 value 1.562699\n",
      "iter 300 value 1.513434\n",
      "iter 310 value 1.477166\n",
      "iter 320 value 1.444268\n",
      "iter 330 value 1.414834\n",
      "iter 340 value 1.397051\n",
      "iter 350 value 1.371017\n",
      "iter 360 value 1.342347\n",
      "iter 370 value 1.322947\n",
      "iter 380 value 1.310806\n",
      "iter 390 value 1.303374\n",
      "iter 400 value 1.295739\n",
      "iter 410 value 1.277853\n",
      "iter 420 value 1.267722\n",
      "iter 430 value 1.263239\n",
      "iter 440 value 1.258956\n",
      "iter 450 value 1.255855\n",
      "iter 460 value 1.253138\n",
      "iter 470 value 1.250135\n",
      "iter 480 value 1.246657\n",
      "iter 490 value 1.243580\n",
      "iter 500 value 1.239779\n",
      "iter 510 value 1.236548\n",
      "iter 520 value 1.234200\n",
      "iter 530 value 1.232723\n",
      "iter 540 value 1.231721\n",
      "iter 550 value 1.230948\n",
      "iter 560 value 1.230481\n",
      "iter 570 value 1.230092\n",
      "iter 580 value 1.229638\n",
      "iter 590 value 1.229140\n",
      "iter 600 value 1.228438\n",
      "iter 610 value 1.227055\n",
      "iter 620 value 1.224161\n",
      "iter 630 value 1.217237\n",
      "iter 640 value 1.212928\n",
      "iter 650 value 1.210684\n",
      "iter 660 value 1.209147\n",
      "iter 670 value 1.207989\n",
      "iter 680 value 1.207190\n",
      "iter 690 value 1.206629\n",
      "iter 700 value 1.206067\n",
      "iter 710 value 1.205505\n",
      "iter 720 value 1.204922\n",
      "iter 730 value 1.203908\n",
      "iter 740 value 1.202005\n",
      "iter 750 value 1.200691\n",
      "iter 760 value 1.199602\n",
      "iter 770 value 1.198747\n",
      "iter 780 value 1.197043\n",
      "iter 790 value 1.194763\n",
      "iter 800 value 1.193033\n",
      "iter 810 value 1.191518\n",
      "iter 820 value 1.189768\n",
      "iter 830 value 1.187333\n",
      "iter 840 value 1.185718\n",
      "iter 850 value 1.184003\n",
      "iter 860 value 1.182963\n",
      "iter 870 value 1.181118\n",
      "iter 880 value 1.179490\n",
      "iter 890 value 1.178421\n",
      "iter 900 value 1.177593\n",
      "iter 910 value 1.176659\n",
      "iter 920 value 1.175727\n",
      "iter 930 value 1.175218\n",
      "iter 940 value 1.174764\n",
      "iter 950 value 1.174197\n",
      "iter 960 value 1.173640\n",
      "iter 970 value 1.173276\n",
      "iter 980 value 1.172878\n",
      "iter 990 value 1.172408\n",
      "iter1000 value 1.171769\n",
      "final  value 1.171769 \n",
      "stopped after 1000 iterations\n",
      "[1] \"Number of neurones:  8 Empirical error =  0.0029\"\n",
      "# weights:  388\n",
      "initial  value 1075.040556 \n",
      "iter  10 value 67.218871\n",
      "iter  20 value 36.245272\n",
      "iter  30 value 16.833370\n",
      "iter  40 value 9.020340\n",
      "iter  50 value 5.745531\n",
      "iter  60 value 4.486742\n",
      "iter  70 value 3.404966\n",
      "iter  80 value 2.902537\n",
      "iter  90 value 2.469939\n",
      "iter 100 value 2.082077\n",
      "iter 110 value 1.693862\n",
      "iter 120 value 1.403633\n",
      "iter 130 value 1.180187\n",
      "iter 140 value 1.051333\n",
      "iter 150 value 0.968014\n",
      "iter 160 value 0.888226\n",
      "iter 170 value 0.827549\n",
      "iter 180 value 0.765064\n",
      "iter 190 value 0.716071\n",
      "iter 200 value 0.684307\n",
      "iter 210 value 0.659731\n",
      "iter 220 value 0.635902\n",
      "iter 230 value 0.604202\n",
      "iter 240 value 0.575497\n",
      "iter 250 value 0.559653\n",
      "iter 260 value 0.538691\n",
      "iter 270 value 0.525129\n",
      "iter 280 value 0.513388\n",
      "iter 290 value 0.500182\n",
      "iter 300 value 0.486856\n",
      "iter 310 value 0.467082\n",
      "iter 320 value 0.454956\n",
      "iter 330 value 0.443491\n",
      "iter 340 value 0.427005\n",
      "iter 350 value 0.413573\n",
      "iter 360 value 0.404842\n",
      "iter 370 value 0.395198\n",
      "iter 380 value 0.382887\n",
      "iter 390 value 0.370531\n",
      "iter 400 value 0.358089\n",
      "iter 410 value 0.348017\n",
      "iter 420 value 0.338933\n",
      "iter 430 value 0.330786\n",
      "iter 440 value 0.322711\n",
      "iter 450 value 0.316186\n",
      "iter 460 value 0.311225\n",
      "iter 470 value 0.307932\n",
      "iter 480 value 0.303964\n",
      "iter 490 value 0.300116\n",
      "iter 500 value 0.294438\n",
      "iter 510 value 0.286948\n",
      "iter 520 value 0.281133\n",
      "iter 530 value 0.276107\n",
      "iter 540 value 0.272104\n",
      "iter 550 value 0.268521\n",
      "iter 560 value 0.263925\n",
      "iter 570 value 0.260006\n",
      "iter 580 value 0.256352\n",
      "iter 590 value 0.252570\n",
      "iter 600 value 0.249194\n",
      "iter 610 value 0.246373\n",
      "iter 620 value 0.244551\n",
      "iter 630 value 0.242407\n",
      "iter 640 value 0.238515\n",
      "iter 650 value 0.234393\n",
      "iter 660 value 0.230027\n",
      "iter 670 value 0.226898\n",
      "iter 680 value 0.224380\n",
      "iter 690 value 0.222881\n",
      "iter 700 value 0.221459\n",
      "iter 710 value 0.220517\n",
      "iter 720 value 0.219800\n",
      "iter 730 value 0.218927\n",
      "iter 740 value 0.218043\n",
      "iter 750 value 0.216837\n",
      "iter 760 value 0.215577\n",
      "iter 770 value 0.214172\n",
      "iter 780 value 0.212829\n",
      "iter 790 value 0.212671\n",
      "iter 800 value 0.212381\n",
      "iter 810 value 0.212139\n",
      "iter 820 value 0.211868\n",
      "iter 830 value 0.211621\n",
      "iter 840 value 0.211290\n",
      "iter 850 value 0.210976\n",
      "iter 860 value 0.210614\n",
      "iter 870 value 0.210238\n",
      "iter 880 value 0.209967\n",
      "iter 890 value 0.209586\n",
      "iter 900 value 0.209091\n",
      "iter 910 value 0.208660\n",
      "iter 920 value 0.208231\n",
      "iter 930 value 0.207699\n",
      "iter 940 value 0.206831\n",
      "iter 950 value 0.205784\n",
      "iter 960 value 0.204779\n",
      "iter 970 value 0.203943\n",
      "iter 980 value 0.203332\n",
      "iter 990 value 0.202814\n",
      "iter1000 value 0.202033\n",
      "final  value 0.202033 \n",
      "stopped after 1000 iterations\n",
      "[1] \"Number of neurones:  9 Empirical error =  5e-04\"\n",
      "# weights:  431\n",
      "initial  value 350.770331 \n",
      "iter  10 value 55.029465\n",
      "iter  20 value 26.250856\n",
      "iter  30 value 14.154954\n",
      "iter  40 value 8.053323\n",
      "iter  50 value 4.889926\n",
      "iter  60 value 3.276394\n",
      "iter  70 value 2.626151\n",
      "iter  80 value 2.159424\n",
      "iter  90 value 1.696552\n",
      "iter 100 value 1.394363\n",
      "iter 110 value 1.204241\n",
      "iter 120 value 1.009473\n",
      "iter 130 value 0.861020\n",
      "iter 140 value 0.744791\n",
      "iter 150 value 0.636500\n",
      "iter 160 value 0.555424\n",
      "iter 170 value 0.494177\n",
      "iter 180 value 0.425920\n",
      "iter 190 value 0.356100\n",
      "iter 200 value 0.302373\n",
      "iter 210 value 0.271841\n",
      "iter 220 value 0.246535\n",
      "iter 230 value 0.229541\n",
      "iter 240 value 0.217643\n",
      "iter 250 value 0.207623\n",
      "iter 260 value 0.199901\n",
      "iter 270 value 0.191495\n",
      "iter 280 value 0.180069\n",
      "iter 290 value 0.170187\n",
      "iter 300 value 0.163130\n",
      "iter 310 value 0.156002\n",
      "iter 320 value 0.150107\n",
      "iter 330 value 0.144348\n",
      "iter 340 value 0.138356\n",
      "iter 350 value 0.132534\n",
      "iter 360 value 0.127839\n",
      "iter 370 value 0.124769\n",
      "iter 380 value 0.122137\n",
      "iter 390 value 0.119011\n",
      "iter 400 value 0.115979\n",
      "iter 410 value 0.112960\n",
      "iter 420 value 0.110002\n",
      "iter 430 value 0.107738\n",
      "iter 440 value 0.105348\n",
      "iter 450 value 0.103057\n",
      "iter 460 value 0.101473\n",
      "iter 470 value 0.100151\n",
      "iter 480 value 0.099136\n",
      "iter 490 value 0.098079\n",
      "iter 500 value 0.096717\n",
      "iter 510 value 0.095534\n",
      "iter 520 value 0.094773\n",
      "iter 530 value 0.094248\n",
      "iter 540 value 0.093913\n",
      "iter 550 value 0.093669\n",
      "iter 560 value 0.093247\n",
      "iter 570 value 0.092797\n",
      "iter 580 value 0.092508\n",
      "iter 590 value 0.092253\n",
      "iter 600 value 0.091982\n",
      "iter 610 value 0.091745\n",
      "iter 620 value 0.091428\n",
      "iter 630 value 0.091060\n",
      "iter 640 value 0.090542\n",
      "iter 650 value 0.090046\n",
      "iter 660 value 0.089679\n",
      "iter 670 value 0.089496\n",
      "iter 680 value 0.089371\n",
      "iter 690 value 0.089191\n",
      "iter 700 value 0.088981\n",
      "iter 710 value 0.088709\n",
      "iter 720 value 0.088318\n",
      "iter 730 value 0.087976\n",
      "iter 740 value 0.087662\n",
      "iter 750 value 0.087531\n",
      "iter 760 value 0.087450\n",
      "iter 770 value 0.087362\n",
      "iter 780 value 0.087263\n",
      "iter 790 value 0.087218\n",
      "iter 800 value 0.087182\n",
      "iter 810 value 0.087158\n",
      "iter 820 value 0.087133\n",
      "iter 830 value 0.087104\n",
      "iter 840 value 0.087079\n",
      "iter 850 value 0.087061\n",
      "iter 860 value 0.087053\n",
      "iter 870 value 0.087048\n",
      "iter 880 value 0.087047\n",
      "iter 890 value 0.087043\n",
      "iter 900 value 0.087036\n",
      "iter 910 value 0.087026\n",
      "iter 920 value 0.087012\n",
      "iter 930 value 0.086997\n",
      "iter 940 value 0.086980\n",
      "iter 950 value 0.086959\n",
      "iter 960 value 0.086937\n",
      "iter 970 value 0.086920\n",
      "iter 980 value 0.086908\n",
      "iter 990 value 0.086900\n",
      "iter1000 value 0.086894\n",
      "final  value 0.086894 \n",
      "stopped after 1000 iterations\n",
      "[1] \"Number of neurones:  10 Empirical error =  2e-04\"\n",
      "# weights:  474\n",
      "initial  value 796.545732 \n",
      "iter  10 value 58.088148\n",
      "iter  20 value 31.253242\n",
      "iter  30 value 15.967410\n",
      "iter  40 value 8.254979\n",
      "iter  50 value 4.876235\n",
      "iter  60 value 3.036271\n",
      "iter  70 value 2.048605\n",
      "iter  80 value 1.531059\n",
      "iter  90 value 1.196972\n",
      "iter 100 value 1.005042\n",
      "iter 110 value 0.797638\n",
      "iter 120 value 0.618031\n",
      "iter 130 value 0.457734\n",
      "iter 140 value 0.374179\n",
      "iter 150 value 0.324063\n",
      "iter 160 value 0.286479\n",
      "iter 170 value 0.252572\n",
      "iter 180 value 0.226419\n",
      "iter 190 value 0.204203\n",
      "iter 200 value 0.182711\n",
      "iter 210 value 0.155001\n",
      "iter 220 value 0.124931\n",
      "iter 230 value 0.099965\n",
      "iter 240 value 0.085139\n",
      "iter 250 value 0.072679\n",
      "iter 260 value 0.066052\n",
      "iter 270 value 0.059919\n",
      "iter 280 value 0.053549\n",
      "iter 290 value 0.048160\n",
      "iter 300 value 0.044157\n",
      "iter 310 value 0.040853\n",
      "iter 320 value 0.037734\n",
      "iter 330 value 0.035435\n",
      "iter 340 value 0.032621\n",
      "iter 350 value 0.029320\n",
      "iter 360 value 0.027787\n",
      "iter 370 value 0.025859\n",
      "iter 380 value 0.023866\n",
      "iter 390 value 0.022257\n",
      "iter 400 value 0.020685\n",
      "iter 410 value 0.019296\n",
      "iter 420 value 0.017614\n",
      "iter 430 value 0.015864\n",
      "iter 440 value 0.013833\n",
      "iter 450 value 0.011823\n",
      "iter 460 value 0.010303\n",
      "iter 470 value 0.008953\n",
      "iter 480 value 0.007932\n",
      "iter 490 value 0.007302\n",
      "iter 500 value 0.006866\n",
      "iter 510 value 0.006443\n",
      "iter 520 value 0.005919\n",
      "iter 530 value 0.005478\n",
      "iter 540 value 0.005055\n",
      "iter 550 value 0.004747\n",
      "iter 560 value 0.004461\n",
      "iter 570 value 0.004282\n",
      "iter 580 value 0.004151\n",
      "iter 590 value 0.004004\n",
      "iter 600 value 0.003875\n",
      "iter 610 value 0.003752\n",
      "iter 620 value 0.003579\n",
      "iter 630 value 0.003402\n",
      "iter 640 value 0.003220\n",
      "iter 650 value 0.003081\n",
      "iter 660 value 0.002915\n",
      "iter 670 value 0.002770\n",
      "iter 680 value 0.002630\n",
      "iter 690 value 0.002471\n",
      "iter 700 value 0.002304\n",
      "iter 710 value 0.002166\n",
      "iter 720 value 0.002064\n",
      "iter 730 value 0.001941\n",
      "iter 740 value 0.001826\n",
      "iter 750 value 0.001715\n",
      "iter 760 value 0.001604\n",
      "iter 770 value 0.001471\n",
      "iter 780 value 0.001354\n",
      "iter 790 value 0.001218\n",
      "iter 800 value 0.001105\n",
      "iter 810 value 0.000984\n",
      "iter 820 value 0.000853\n",
      "iter 830 value 0.000745\n",
      "iter 840 value 0.000685\n",
      "iter 850 value 0.000623\n",
      "iter 860 value 0.000561\n",
      "iter 870 value 0.000499\n",
      "iter 880 value 0.000453\n",
      "iter 890 value 0.000410\n",
      "iter 900 value 0.000365\n",
      "iter 910 value 0.000327\n",
      "iter 920 value 0.000295\n",
      "iter 930 value 0.000268\n",
      "iter 940 value 0.000244\n",
      "iter 950 value 0.000231\n",
      "iter 960 value 0.000230\n",
      "iter 970 value 0.000230\n",
      "iter 980 value 0.000228\n",
      "iter 990 value 0.000226\n",
      "iter1000 value 0.000223\n",
      "final  value 0.000223 \n",
      "stopped after 1000 iterations\n",
      "[1] \"Number of neurones:  11 Empirical error =  0\"\n",
      "# weights:  517\n",
      "initial  value 993.070344 \n",
      "iter  10 value 58.952540\n",
      "iter  20 value 34.213020\n",
      "iter  30 value 15.092096\n",
      "iter  40 value 7.056287\n",
      "iter  50 value 3.617536\n",
      "iter  60 value 2.007728\n",
      "iter  70 value 1.304136\n",
      "iter  80 value 0.945029\n",
      "iter  90 value 0.678715\n",
      "iter 100 value 0.473163\n",
      "iter 110 value 0.348378\n",
      "iter 120 value 0.245589\n",
      "iter 130 value 0.183065\n",
      "iter 140 value 0.135544\n",
      "iter 150 value 0.108288\n",
      "iter 160 value 0.086072\n",
      "iter 170 value 0.070467\n",
      "iter 180 value 0.056251\n",
      "iter 190 value 0.046822\n",
      "iter 200 value 0.037650\n",
      "iter 210 value 0.031048\n",
      "iter 220 value 0.025792\n",
      "iter 230 value 0.020043\n",
      "iter 240 value 0.016454\n",
      "iter 250 value 0.014073\n",
      "iter 260 value 0.011941\n",
      "iter 270 value 0.009977\n",
      "iter 280 value 0.008208\n",
      "iter 290 value 0.006751\n",
      "iter 300 value 0.005383\n",
      "iter 310 value 0.004333\n",
      "iter 320 value 0.003536\n",
      "iter 330 value 0.002461\n",
      "iter 340 value 0.001653\n",
      "iter 350 value 0.001068\n",
      "iter 360 value 0.000703\n",
      "iter 370 value 0.000534\n",
      "iter 380 value 0.000356\n",
      "iter 390 value 0.000208\n",
      "iter 400 value 0.000124\n",
      "final  value 0.000095 \n",
      "converged\n",
      "[1] \"Number of neurones:  12 Empirical error =  0\"\n",
      "# weights:  560\n",
      "initial  value 466.593387 \n",
      "iter  10 value 59.285360\n",
      "iter  20 value 26.936359\n",
      "iter  30 value 14.312044\n",
      "iter  40 value 6.043232\n",
      "iter  50 value 3.307159\n",
      "iter  60 value 2.023886\n",
      "iter  70 value 1.329003\n",
      "iter  80 value 0.890065\n",
      "iter  90 value 0.618536\n",
      "iter 100 value 0.444930\n",
      "iter 110 value 0.284029\n",
      "iter 120 value 0.196083\n",
      "iter 130 value 0.121594\n",
      "iter 140 value 0.069199\n",
      "iter 150 value 0.042732\n",
      "iter 160 value 0.027362\n",
      "iter 170 value 0.017344\n",
      "iter 180 value 0.011323\n",
      "iter 190 value 0.008300\n",
      "iter 200 value 0.006323\n",
      "iter 210 value 0.005045\n",
      "iter 220 value 0.004072\n",
      "iter 230 value 0.003248\n",
      "iter 240 value 0.002435\n",
      "iter 250 value 0.001879\n",
      "iter 260 value 0.001456\n",
      "iter 270 value 0.001102\n",
      "iter 280 value 0.000731\n",
      "iter 290 value 0.000432\n",
      "iter 300 value 0.000292\n",
      "iter 310 value 0.000204\n",
      "iter 320 value 0.000128\n",
      "final  value 0.000097 \n",
      "converged\n",
      "[1] \"Number of neurones:  13 Empirical error =  0\"\n",
      "# weights:  603\n",
      "initial  value 392.989682 \n",
      "iter  10 value 59.124491\n",
      "iter  20 value 25.143357\n",
      "iter  30 value 11.906834\n",
      "iter  40 value 4.059063\n",
      "iter  50 value 1.963353\n",
      "iter  60 value 1.062352\n",
      "iter  70 value 0.643911\n",
      "iter  80 value 0.432386\n",
      "iter  90 value 0.266697\n",
      "iter 100 value 0.154278\n",
      "iter 110 value 0.088273\n",
      "iter 120 value 0.050260\n",
      "iter 130 value 0.027757\n",
      "iter 140 value 0.014407\n",
      "iter 150 value 0.008907\n",
      "iter 160 value 0.006178\n",
      "iter 170 value 0.004342\n",
      "iter 180 value 0.002634\n",
      "iter 190 value 0.001471\n",
      "iter 200 value 0.000866\n",
      "iter 210 value 0.000544\n",
      "iter 220 value 0.000371\n",
      "iter 230 value 0.000226\n",
      "iter 240 value 0.000144\n",
      "final  value 0.000097 \n",
      "converged\n",
      "[1] \"Number of neurones:  14 Empirical error =  0\"\n",
      "# weights:  646\n",
      "initial  value 1990.179824 \n",
      "iter  10 value 59.714037\n",
      "iter  20 value 26.840170\n",
      "iter  30 value 14.869949\n",
      "iter  40 value 8.046000\n",
      "iter  50 value 4.736304\n",
      "iter  60 value 2.198457\n",
      "iter  70 value 1.236147\n",
      "iter  80 value 0.727764\n",
      "iter  90 value 0.414606\n",
      "iter 100 value 0.226324\n",
      "iter 110 value 0.135440\n",
      "iter 120 value 0.089930\n",
      "iter 130 value 0.058593\n",
      "iter 140 value 0.036019\n",
      "iter 150 value 0.019311\n",
      "iter 160 value 0.009578\n",
      "iter 170 value 0.004776\n",
      "iter 180 value 0.002573\n",
      "iter 190 value 0.001443\n",
      "iter 200 value 0.000832\n",
      "iter 210 value 0.000476\n",
      "iter 220 value 0.000246\n",
      "iter 230 value 0.000116\n",
      "final  value 0.000098 \n",
      "converged\n",
      "[1] \"Number of neurones:  15 Empirical error =  0\"\n",
      "# weights:  689\n",
      "initial  value 966.954168 \n",
      "iter  10 value 65.513912\n",
      "iter  20 value 28.386495\n",
      "iter  30 value 13.744726\n",
      "iter  40 value 4.949520\n",
      "iter  50 value 2.206878\n",
      "iter  60 value 1.093406\n",
      "iter  70 value 0.566677\n",
      "iter  80 value 0.335575\n",
      "iter  90 value 0.202570\n",
      "iter 100 value 0.103583\n",
      "iter 110 value 0.053706\n",
      "iter 120 value 0.031349\n",
      "iter 130 value 0.020771\n",
      "iter 140 value 0.016073\n",
      "iter 150 value 0.013149\n",
      "iter 160 value 0.009468\n",
      "iter 170 value 0.006345\n",
      "iter 180 value 0.003869\n",
      "iter 190 value 0.002371\n",
      "iter 200 value 0.001409\n",
      "iter 210 value 0.000691\n",
      "iter 220 value 0.000251\n",
      "final  value 0.000094 \n",
      "converged\n",
      "[1] \"Number of neurones:  16 Empirical error =  0\"\n",
      "# weights:  732\n",
      "initial  value 1363.971664 \n",
      "iter  10 value 81.914741\n",
      "iter  20 value 25.880424\n",
      "iter  30 value 13.087432\n",
      "iter  40 value 4.602190\n",
      "iter  50 value 2.010656\n",
      "iter  60 value 1.091932\n",
      "iter  70 value 0.584338\n",
      "iter  80 value 0.347132\n",
      "iter  90 value 0.175405\n",
      "iter 100 value 0.097737\n",
      "iter 110 value 0.047963\n",
      "iter 120 value 0.024248\n",
      "iter 130 value 0.012033\n",
      "iter 140 value 0.005803\n",
      "iter 150 value 0.002710\n",
      "iter 160 value 0.001254\n",
      "iter 170 value 0.000650\n",
      "iter 180 value 0.000382\n",
      "iter 190 value 0.000253\n",
      "iter 200 value 0.000154\n",
      "iter 210 value 0.000104\n",
      "final  value 0.000099 \n",
      "converged\n",
      "[1] \"Number of neurones:  17 Empirical error =  0\"\n",
      "# weights:  775\n",
      "initial  value 1427.829209 \n",
      "iter  10 value 68.366185\n",
      "iter  20 value 27.552646\n",
      "iter  30 value 13.158296\n",
      "iter  40 value 4.334917\n",
      "iter  50 value 1.564671\n",
      "iter  60 value 0.571927\n",
      "iter  70 value 0.227028\n",
      "iter  80 value 0.118059\n",
      "iter  90 value 0.065587\n",
      "iter 100 value 0.027693\n",
      "iter 110 value 0.011652\n",
      "iter 120 value 0.004517\n",
      "iter 130 value 0.001905\n",
      "iter 140 value 0.000940\n",
      "iter 150 value 0.000565\n",
      "iter 160 value 0.000377\n",
      "iter 170 value 0.000262\n",
      "iter 180 value 0.000165\n",
      "final  value 0.000097 \n",
      "converged\n",
      "[1] \"Number of neurones:  18 Empirical error =  0\"\n",
      "# weights:  818\n",
      "initial  value 570.157775 \n",
      "iter  10 value 58.331746\n",
      "iter  20 value 23.630644\n",
      "iter  30 value 11.859763\n",
      "iter  40 value 4.688688\n",
      "iter  50 value 1.557741\n",
      "iter  60 value 0.656185\n",
      "iter  70 value 0.317019\n",
      "iter  80 value 0.170956\n",
      "iter  90 value 0.090198\n",
      "iter 100 value 0.044887\n",
      "iter 110 value 0.021685\n",
      "iter 120 value 0.010963\n",
      "iter 130 value 0.007217\n",
      "iter 140 value 0.004634\n",
      "iter 150 value 0.002665\n",
      "iter 160 value 0.001572\n",
      "iter 170 value 0.000838\n",
      "iter 180 value 0.000491\n",
      "iter 190 value 0.000342\n",
      "iter 200 value 0.000265\n",
      "iter 210 value 0.000174\n",
      "final  value 0.000098 \n",
      "converged\n",
      "[1] \"Number of neurones:  19 Empirical error =  0\"\n",
      "# weights:  861\n",
      "initial  value 1557.903459 \n",
      "iter  10 value 77.720233\n",
      "iter  20 value 24.027381\n",
      "iter  30 value 9.504022\n",
      "iter  40 value 4.531208\n",
      "iter  50 value 2.838954\n",
      "iter  60 value 1.077895\n",
      "iter  70 value 0.394872\n",
      "iter  80 value 0.156632\n",
      "iter  90 value 0.055908\n",
      "iter 100 value 0.027041\n",
      "iter 110 value 0.018984\n",
      "iter 120 value 0.014714\n",
      "iter 130 value 0.011327\n",
      "iter 140 value 0.007234\n",
      "iter 150 value 0.003063\n",
      "iter 160 value 0.000954\n",
      "iter 170 value 0.000306\n",
      "iter 180 value 0.000104\n",
      "iter 180 value 0.000097\n",
      "iter 180 value 0.000097\n",
      "final  value 0.000097 \n",
      "converged\n",
      "[1] \"Number of neurones:  20 Empirical error =  0\"\n",
      "# weights:  904\n",
      "initial  value 818.443003 \n",
      "iter  10 value 60.175493\n",
      "iter  20 value 21.786680\n",
      "iter  30 value 10.987400\n",
      "iter  40 value 5.566205\n",
      "iter  50 value 1.485068\n",
      "iter  60 value 0.576883\n",
      "iter  70 value 0.210587\n",
      "iter  80 value 0.089196\n",
      "iter  90 value 0.040588\n",
      "iter 100 value 0.013039\n",
      "iter 110 value 0.004661\n",
      "iter 120 value 0.001638\n",
      "iter 130 value 0.000657\n",
      "iter 140 value 0.000364\n",
      "iter 150 value 0.000158\n",
      "final  value 0.000096 \n",
      "converged\n",
      "[1] \"Number of neurones:  21 Empirical error =  0\"\n",
      "# weights:  947\n",
      "initial  value 880.574203 \n",
      "iter  10 value 58.597191\n",
      "iter  20 value 18.877571\n",
      "iter  30 value 8.271942\n",
      "iter  40 value 3.889746\n",
      "iter  50 value 1.296045\n",
      "iter  60 value 0.543352\n",
      "iter  70 value 0.218595\n",
      "iter  80 value 0.102138\n",
      "iter  90 value 0.049873\n",
      "iter 100 value 0.022389\n",
      "iter 110 value 0.008396\n",
      "iter 120 value 0.003152\n",
      "iter 130 value 0.001328\n",
      "iter 140 value 0.000621\n",
      "iter 150 value 0.000281\n",
      "iter 160 value 0.000168\n",
      "iter 170 value 0.000104\n",
      "iter 170 value 0.000100\n",
      "iter 170 value 0.000099\n",
      "final  value 0.000099 \n",
      "converged\n",
      "[1] \"Number of neurones:  22 Empirical error =  0\"\n"
     ]
    }
   ],
   "source": [
    "DS<-cbind(X.scale,SalePrice=Y.scale)\n",
    "empirical_error <- numeric(20)\n",
    "\n",
    "for (i in 1:20){\n",
    "    #model.neural[i] <- nnet(SalePrice~.,DS, size = i, linout=T, maxit = 1000)\n",
    "    Y.hat <- predict(nnet(SalePrice~.,DS, size = i, linout=T, maxit = 1000),X.scale)\n",
    "\n",
    "    empirical_error[i] <- mean((Y.hat-Y.scale)^2) \n",
    "    print(paste(\"Number of neurones: \", i, \"Empirical error = \",round(empirical_error[i],digits=4)))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter \"size\"\n",
    "An analysis of the number of neurones is done. The maximum of neurones used is 20 as more return an error (too many weights).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>0.179332902548681</li>\n",
       "\t<li>NA</li>\n",
       "\t<li>NA</li>\n",
       "\t<li>NA</li>\n",
       "\t<li>0.014377193726611</li>\n",
       "\t<li>0.0129928382919396</li>\n",
       "\t<li>0.00695854002209319</li>\n",
       "\t<li>0.00292942358495676</li>\n",
       "\t<li>0.000505083500198126</li>\n",
       "\t<li>0.000217234027011571</li>\n",
       "\t<li>5.58061044076295e-07</li>\n",
       "\t<li>2.36626390948462e-07</li>\n",
       "\t<li>2.41913928238747e-07</li>\n",
       "\t<li>2.4324409183602e-07</li>\n",
       "\t<li>2.4518662006099e-07</li>\n",
       "\t<li>2.3434814437606e-07</li>\n",
       "\t<li>2.46366423139693e-07</li>\n",
       "\t<li>2.43385476345626e-07</li>\n",
       "\t<li>2.44030155532961e-07</li>\n",
       "\t<li>2.43035407984296e-07</li>\n",
       "\t<li>2.40195640655436e-07</li>\n",
       "\t<li>2.4870361087822e-07</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0.179332902548681\n",
       "\\item NA\n",
       "\\item NA\n",
       "\\item NA\n",
       "\\item 0.014377193726611\n",
       "\\item 0.0129928382919396\n",
       "\\item 0.00695854002209319\n",
       "\\item 0.00292942358495676\n",
       "\\item 0.000505083500198126\n",
       "\\item 0.000217234027011571\n",
       "\\item 5.58061044076295e-07\n",
       "\\item 2.36626390948462e-07\n",
       "\\item 2.41913928238747e-07\n",
       "\\item 2.4324409183602e-07\n",
       "\\item 2.4518662006099e-07\n",
       "\\item 2.3434814437606e-07\n",
       "\\item 2.46366423139693e-07\n",
       "\\item 2.43385476345626e-07\n",
       "\\item 2.44030155532961e-07\n",
       "\\item 2.43035407984296e-07\n",
       "\\item 2.40195640655436e-07\n",
       "\\item 2.4870361087822e-07\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0.179332902548681\n",
       "2. NA\n",
       "3. NA\n",
       "4. NA\n",
       "5. 0.014377193726611\n",
       "6. 0.0129928382919396\n",
       "7. 0.00695854002209319\n",
       "8. 0.00292942358495676\n",
       "9. 0.000505083500198126\n",
       "10. 0.000217234027011571\n",
       "11. 5.58061044076295e-07\n",
       "12. 2.36626390948462e-07\n",
       "13. 2.41913928238747e-07\n",
       "14. 2.4324409183602e-07\n",
       "15. 2.4518662006099e-07\n",
       "16. 2.3434814437606e-07\n",
       "17. 2.46366423139693e-07\n",
       "18. 2.43385476345626e-07\n",
       "19. 2.44030155532961e-07\n",
       "20. 2.43035407984296e-07\n",
       "21. 2.40195640655436e-07\n",
       "22. 2.4870361087822e-07\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] 1.793329e-01           NA           NA           NA 1.437719e-02\n",
       " [6] 1.299284e-02 6.958540e-03 2.929424e-03 5.050835e-04 2.172340e-04\n",
       "[11] 5.580610e-07 2.366264e-07 2.419139e-07 2.432441e-07 2.451866e-07\n",
       "[16] 2.343481e-07 2.463664e-07 2.433855e-07 2.440302e-07 2.430354e-07\n",
       "[21] 2.401956e-07 2.487036e-07"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "empirical_error\n",
    "#juste un save pour les tests\n",
    "#empirical_error <- c(0.0143671130358607, 0.015019445363705, 0.00821834907673351, 0.0054200745645435, 0.00142228029226354, 5.25161597874794e-05, 1.52899010549121e-06, 2.47317346679477e-07, 2.37597648006074e-07, 2.47841099771862e-07, 2.26350153783801e-07, 2.30675129791945e-07, 2.44205643742954e-07, 2.4713486169669e-07, 2.40667851762e-07, 2.31459671090645e-07, 2.39175596505192e-07, 2.24902711484879e-07) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(c(5:22),empirical_error[5:22], xlab = \"Number of neurones\", ylab = \"Empirical error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first view it seems like a size > 10 gives the best result. As the computational time incrases with the number of neurones, its important to find the smallest size that gives good result. We will now asses the 10-fold cross validation for the 10 models (size 10 to 20) to determine if they are good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot(c(12:22),empirical_error[8:18], xlab = \"Number of neurones\", ylab = \"Empirical error\")  # zoom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean.size<-numeric(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "size.CV<-floor(N/10)\n",
    "\n",
    "CV.err<-numeric(10)\n",
    "\n",
    "for (j in 10:20){\n",
    "    \n",
    "    for (i in 1:10) {\n",
    "        # 1/10 for testing\n",
    "         i.ts<-(((i-1)*size.CV+1):(i*size.CV))  ### i.ts = indices of the test set for the i-th fold\n",
    "         X.ts<-X[i.ts,]  \n",
    "         Y.ts<-Y[i.ts]    \n",
    "\n",
    "        #9/10 for training\n",
    "         i.tr<-setdiff(1:N,i.ts)                ###i.tr = indices of the training set for the i-th fold\n",
    "         X.tr<-X[i.tr,]\n",
    "         Y.tr<-Y[i.tr]         \n",
    "\n",
    "        #scaling\n",
    "         X.tr.mean <- colMeans(X.tr)\n",
    "         X.tr.sd <- apply(X.tr,2,sd)\n",
    "        Y.tr.mean <- mean(Y.tr)\n",
    "        Y.tr.sd <- sd(Y.tr)\n",
    "\n",
    "        Y.tr <- cbind(Y.tr - Y.tr.mean)/Y.tr.sd \n",
    "        X.tr <- t(apply(sweep(X.tr,2,X.tr.mean,\"-\"), 1, function(x) x/X.tr.sd))\n",
    "\n",
    "        #scaling the testing test by the same scaling as of the training set\n",
    "        X.ts <- t(apply(sweep(X.ts,2,X.tr.mean,\"-\"), 1, function(x) x/X.tr.sd))\n",
    "        Y.ts <- cbind(Y.ts - Y.tr.mean)/Y.tr.sd \n",
    "\n",
    "        X.ts <-data.frame(X.ts)\n",
    "        X.tr <-data.frame(X.tr)                       \n",
    "\n",
    "        DS <- cbind(X.tr,SalePrice=Y.tr)\n",
    "        DS<- DS[,colSums(!is.na(DS)) > 0]    \n",
    "            \n",
    "        model.neural <- nnet(SalePrice~.,DS, size = j, linout=T, maxit = 1000)\n",
    "\n",
    "        Y.hat.ts<- predict(model.neural,X.ts)\n",
    "\n",
    "        CV.err[i]<-mean((Y.hat.ts-Y.ts)^2)\n",
    "    }\n",
    "    mean.size[j] <- round( mean(CV.err),digits=4 )\n",
    "    print(paste(\"Number of neurones: \", j, \" CV error = \",mean.size[j-9], \" std dev = \",round(sd(CV.err),digits=4)))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(c(10:20), mean.size(10:20), xlabel = \"Number of neurones\" , ylabel= \"Cross validation error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the CV error is far from the ~0 we had before. This is because the model was overfitting the set. Let's see what kind of results we get for the CV with a smaller network size (<9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "size.CV<-floor(N/10)\n",
    "\n",
    "for (j in 1:9){\n",
    "    \n",
    "    for (i in 1:10) {\n",
    "        # 1/10 for testing\n",
    "         i.ts<-(((i-1)*size.CV+1):(i*size.CV))  ### i.ts = indices of the test set for the i-th fold\n",
    "         X.ts<-X[i.ts,]  \n",
    "         Y.ts<-Y[i.ts]    \n",
    "\n",
    "        #9/10 for training\n",
    "         i.tr<-setdiff(1:N,i.ts)                ###i.tr = indices of the training set for the i-th fold\n",
    "         X.tr<-X[i.tr,]\n",
    "         Y.tr<-Y[i.tr]         \n",
    "\n",
    "        #scaling\n",
    "         X.tr.mean <- colMeans(X.tr)\n",
    "         X.tr.sd <- apply(X.tr,2,sd)\n",
    "        Y.tr.mean <- mean(Y.tr)\n",
    "        Y.tr.sd <- sd(Y.tr)\n",
    "\n",
    "        Y.tr <- cbind(Y.tr - Y.tr.mean)/Y.tr.sd \n",
    "        X.tr <- t(apply(sweep(X.tr,2,X.tr.mean,\"-\"), 1, function(x) x/X.tr.sd))\n",
    "\n",
    "        #scaling the testing test by the same scaling as of the training set\n",
    "        X.ts <- t(apply(sweep(X.ts,2,X.tr.mean,\"-\"), 1, function(x) x/X.tr.sd))\n",
    "        Y.ts <- cbind(Y.ts - Y.tr.mean)/Y.tr.sd \n",
    "\n",
    "        X.ts <-data.frame(X.ts)\n",
    "        X.tr <-data.frame(X.tr)                       \n",
    "\n",
    "         DS <- cbind(X.tr,SalePrice=Y.tr)\n",
    "         DS<- DS[,colSums(!is.na(DS)) > 0]    \n",
    "            \n",
    "         model.neural <- nnet(SalePrice~.,DS, size = j, linout=T, maxit = 1000)\n",
    "\n",
    "         Y.hat.ts<- predict(model.neural,X.ts)\n",
    "\n",
    "         CV.err[i]<-mean((Y.hat.ts-Y.ts)^2)\n",
    "        }\n",
    "    mean.size[j] <- round( mean(CV.err),digits=4 )\n",
    "    print(paste(\"Number of neurones: \", j, \" CV error = \",mean.size[j], \" std dev = \",round(sd(CV.err),digits=4)))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(c(3:9), mean.size[1:9], xlabel = \"Number of neurones\" , ylabel= \"Cross validation error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean.size[1:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble techniques : Combination of models strategy\n",
    "> Methodology and main results\n",
    "\n",
    "> The text should mention the different models taken into consideration as well as the techniques used for the combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size.CV<-floor(N/10)\n",
    "R<-20   # R models\n",
    "\n",
    "CV.err<-numeric(10)\n",
    "\n",
    "for (i in 1:10) {\n",
    "     i.ts<-(((i-1)*size.CV+1):(i*size.CV))  \n",
    "     X.ts<-X[i.ts,]  \n",
    "     Y.ts<-Y[i.ts]  \n",
    "     \n",
    "     \n",
    "     i.tr<-setdiff(1:N,i.ts)                \n",
    "    \n",
    "     Y.hat.ts.R<-matrix(0,nrow=nrow(X.ts),ncol=R)\n",
    "    \n",
    "     for (r in 1:R) {\n",
    "         i.tr.resample<-sample(i.tr,rep=T)  #rep = replace\n",
    "         X.tr<-X[i.tr.resample,]\n",
    "         Y.tr<-Y[i.tr.resample]                          \n",
    "     \n",
    "         DS<-cbind(X.tr,SalePrice=Y.tr)\n",
    "    \n",
    "         model<- lm(SalePrice~.,DS)\n",
    "        \n",
    "         Y.hat.ts.R[,r]<- predict(model,X.ts)\n",
    "     \n",
    "     }\n",
    "    \n",
    "     Y.hat.ts<-apply(Y.hat.ts.R,1,mean) \n",
    "     CV.err[i]<-mean((Y.hat.ts-Y.ts)^2)\n",
    "     }\n",
    "\n",
    "print(paste(\"CV error=\",round(mean(CV.err),digits=4), \" ; std dev=\",round(sd(CV.err),digits=4)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion and conclusion: \n",
    "> Summary of your work, and discussion of what worked well, not well, why, what insights you got from the analyses you made. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R 3.3",
   "language": "R",
   "name": "ir33"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
